{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are running this in Google Colab, make sure you install the following dependencies:\n",
        "\n",
        "```bash\n",
        "!pip3 install torchrl\n",
        "!pip3 install vmas\n",
        "!pip3 install tqdm\n",
        "```\n",
        "Proximal Policy Optimization (PPO) is a policy-gradient algorithm where a\n",
        "batch of data is being collected and directly consumed to train the policy to maximise\n",
        "the expected return given some proximality constraints. You can think of it\n",
        "as a sophisticated version of [REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf),\n",
        "the foundational policy-optimization algorithm. For more information, see the\n",
        "[Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) paper.\n",
        "\n",
        "This type of algorithms is usually trained *on-policy*. This means that, at every learning iteration, we have a\n",
        "**sampling** and a **training** phase. In the **sampling** phase of iteration $t$, rollouts are collected\n",
        "from agents' interactions in the environment using the current policies $\\mathbf{\\pi}_t$.\n",
        "In the **training** phase, all the collected rollouts are immediately fed to the training process to perform\n",
        "backpropagation. This leads to updated policies which are then used again for sampling.\n",
        "The execution of this process in a loop constitutes *on-policy learning*.\n",
        "\n",
        ".. figure:: https://pytorch.s3.amazonaws.com/torchrl/github-artifacts/img/on_policy_vmas.png\n",
        "   :alt: On-policy learning\n",
        "\n",
        "   On-policy learning\n",
        "\n",
        "\n",
        "In the training phase of the PPO algorithm, a *critic* is used to estimate the goodness of the actions\n",
        "taken by the policy. The critic learns to approximate the value (mean discounted return) of a specific state.\n",
        "The PPO loss then compares the actual return obtained by the policy to the one estimated by the critic to determine\n",
        "the advantage of the action taken and guide the policy optimization.\n",
        "\n",
        "In multi-agent settings, things are a bit different. We now have multiple policies $\\mathbf{\\pi}$,\n",
        "one for each agent. Policies are typically local and decentralised. This means that\n",
        "the policy for a single agent will output an action for that agent based only on its observation.\n",
        "In the MARL literature, this is referred to as **decentralised execution**.\n",
        "On the other hand, different formulations exist for the critic, mainly:\n",
        "\n",
        "- In [MAPPO](https://arxiv.org/abs/2103.01955) the critic is centralised and takes as input the global state\n",
        "  of the system. This can be a global observation or simply the concatenation of the agents' observation. MAPPO\n",
        "  can be used in contexts where **centralised training** is performed as it needs access to global information.\n",
        "- In [IPPO](https://arxiv.org/abs/2011.09533) the critic takes as input just the observation of the respective agent,\n",
        "  exactly like the policy. This allows **decentralised training** as both the critic and the policy will only need local\n",
        "  information to compute their outputs.\n",
        "\n",
        "Centralised critics help overcome the non-stationary of multiple agents learning concurrently, but,\n",
        "on the other hand, they may be impacted by their large input space.\n",
        "In this tutorial, we will be able to train both formulations, and we will also discuss how\n",
        "parameter-sharing (the practice of sharing the network parameters across the agents) impacts each.\n",
        "\n",
        "This tutorial is structured as follows:\n",
        "\n",
        "1. First, we will define a set of hyperparameters we will be using.\n",
        "\n",
        "2. Next, we will create a vectorized multi-agent environment, using TorchRL's\n",
        "   wrapper for the VMAS simulator.\n",
        "\n",
        "3. Next, we will design the policy and the critic networks, discussing the impact of the various choices on\n",
        "   parameter sharing and critic centralization.\n",
        "\n",
        "4. Next, we will create the sampling collector and the replay buffer.\n",
        "\n",
        "5. Finally, we will run our training loop and analyse the results.\n",
        "\n",
        "If you are running this in Colab or in a machine with a GUI, you will also have the option\n",
        "to render and visualise your own trained policy prior and after training.\n",
        "\n",
        "Let's import our dependencies\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Torch\n",
        "import torch\n",
        "\n",
        "# Tensordict modules\n",
        "from tensordict.nn import set_composite_lp_aggregate, TensorDictModule\n",
        "from tensordict.nn.distributions import NormalParamExtractor\n",
        "from torch import multiprocessing\n",
        "\n",
        "# Data collection\n",
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data.replay_buffers import ReplayBuffer\n",
        "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
        "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
        "\n",
        "# Env\n",
        "from torchrl.envs import RewardSum, TransformedEnv\n",
        "from torchrl.envs.libs.vmas import VmasEnv\n",
        "from torchrl.envs.utils import check_env_specs\n",
        "\n",
        "# Multi-agent network\n",
        "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
        "\n",
        "# Loss\n",
        "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
        "\n",
        "# Utils\n",
        "torch.manual_seed(0)\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Hyperparameters\n",
        "\n",
        "We set the hyperparameters for our tutorial.\n",
        "Depending on the resources\n",
        "available, one may choose to execute the policy and the simulator on GPU or on another\n",
        "device.\n",
        "You can tune some of these values to adjust the computational requirements.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Devices\n",
        "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
        "device = (\n",
        "    torch.device(0)\n",
        "    if torch.cuda.is_available() and not is_fork\n",
        "    else torch.device(\"cuda\")\n",
        ")\n",
        "vmas_device = device  # The device where the simulator is run (VMAS can run on GPU)\n",
        "\n",
        "# Sampling\n",
        "frames_per_batch = 6_000  # Number of team frames collected per training iteration\n",
        "n_iters = 5  # Number of sampling and training iterations\n",
        "total_frames = frames_per_batch * n_iters\n",
        "\n",
        "# Training\n",
        "num_epochs = 30  # Number of optimization steps per training iteration\n",
        "minibatch_size = 400  # Size of the mini-batches in each optimization step\n",
        "lr = 3e-4  # Learning rate\n",
        "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
        "\n",
        "# PPO\n",
        "clip_epsilon = 0.2  # clip value for PPO loss\n",
        "gamma = 0.99  # discount factor\n",
        "lmbda = 0.9  # lambda for generalised advantage estimation\n",
        "entropy_eps = 1e-4  # coefficient of the entropy term in the PPO loss\n",
        "\n",
        "# disable log-prob aggregation\n",
        "set_composite_lp_aggregate(False).set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment\n",
        "\n",
        "Multi-agent environments simulate multiple agents interacting with the world.\n",
        "TorchRL API allows integrating various types of multi-agent environment flavors.\n",
        "Some examples include environments with shared or individual agent rewards, done flags, and observations.\n",
        "For more information on how the multi-agent environments API works in TorchRL, you can check out the dedicated\n",
        "`doc section <MARL-environment-API>`.\n",
        "\n",
        "The VMAS simulator, in particular, models agents with individual rewards, info, observations, and actions, but\n",
        "with a collective done flag.\n",
        "Furthermore, it uses *vectorization* to perform simulation in a batch.\n",
        "This means that all its state and physics\n",
        "are PyTorch tensors with a first dimension representing the number of parallel environments in a batch.\n",
        "This allows leveraging the Single Instruction Multiple Data (SIMD) paradigm of GPUs and significantly\n",
        "speed up parallel computation by leveraging parallelization in GPU warps. It also means\n",
        "that, when using it in TorchRL, both simulation and training can be run on-device, without ever passing\n",
        "data to the CPU.\n",
        "\n",
        "The multi-agent task we will solve today is *Navigation* (see animated figure above).\n",
        "In *Navigation*, randomly spawned agents\n",
        "(circles with surrounding dots) need to navigate\n",
        "to randomly spawned goals (smaller circles).\n",
        "Agents need to use LIDARs (dots around them) to\n",
        "avoid colliding into each other.\n",
        "Agents act in a 2D continuous world with drag and elastic collisions.\n",
        "Their actions are 2D continuous forces which determine their acceleration.\n",
        "The reward is composed of three terms: a collision penalization, a reward based on the distance to the goal, and a\n",
        "final shared reward given when all agents reach their goal.\n",
        "The distance-based term is computed as the difference in the relative distance\n",
        "between an agent and its goal over two consecutive timesteps.\n",
        "Each agent observes its position,\n",
        "velocity, lidar readings, and relative position to its goal.\n",
        "\n",
        "We will now instantiate the environment.\n",
        "For this tutorial, we will limit the episodes to ``max_steps``, after which the done flag is set. This is\n",
        "functionality is already provided in the VMAS simulator but the TorchRL :class:`~.envs.transforms.StepCount`\n",
        "transform could alternatively be used.\n",
        "We will also use ``num_vmas_envs`` vectorized environments, to leverage batch simulation.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_steps = 100  # Episode steps before done\n",
        "num_vmas_envs = (\n",
        "    frames_per_batch // max_steps\n",
        ")  # Number of vectorized envs. frames_per_batch should be divisible by this number\n",
        "scenario_name = \"navigation\"\n",
        "n_agents = 3\n",
        "\n",
        "env = VmasEnv(\n",
        "    scenario=scenario_name,\n",
        "    num_envs=num_vmas_envs,\n",
        "    continuous_actions=True,  # VMAS supports both continuous and discrete actions\n",
        "    max_steps=max_steps,\n",
        "    device=vmas_device,\n",
        "    # Scenario kwargs\n",
        "    n_agents=n_agents,  # These are custom kwargs that change for each VMAS scenario, see the VMAS repo to know more.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The environment is not only defined by its simulator and transforms, but also\n",
        "by a series of metadata that describe what can be expected during its\n",
        "execution.\n",
        "For efficiency purposes, TorchRL is quite stringent when it comes to\n",
        "environment specs, but you can easily check that your environment specs are\n",
        "adequate.\n",
        "In our example, the :class:`~.envs.libs.vmas.VmasEnv` takes care of setting the proper specs for your env so\n",
        "you should not have to care about this.\n",
        "\n",
        "There are four specs to look at:\n",
        "\n",
        "- ``action_spec`` defines the action space;\n",
        "- ``reward_spec`` defines the reward domain;\n",
        "- ``done_spec`` defines the done domain;\n",
        "- ``observation_spec`` which defines the domain of all other outputs from environment steps;\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "action_spec: Composite(\n",
            "    agents: Composite(\n",
            "        action: BoundedContinuous(\n",
            "            shape=torch.Size([60, 3, 2]),\n",
            "            space=ContinuousBox(\n",
            "                low=Tensor(shape=torch.Size([60, 3, 2]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
            "                high=Tensor(shape=torch.Size([60, 3, 2]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
            "            device=cuda:0,\n",
            "            dtype=torch.float32,\n",
            "            domain=continuous),\n",
            "        device=cuda:0,\n",
            "        shape=torch.Size([60, 3]),\n",
            "        data_cls=None),\n",
            "    device=cuda:0,\n",
            "    shape=torch.Size([60]),\n",
            "    data_cls=None)\n",
            "reward_spec: Composite(\n",
            "    agents: Composite(\n",
            "        reward: UnboundedContinuous(\n",
            "            shape=torch.Size([60, 3, 1]),\n",
            "            space=ContinuousBox(\n",
            "                low=Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
            "                high=Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
            "            device=cuda:0,\n",
            "            dtype=torch.float32,\n",
            "            domain=continuous),\n",
            "        device=cuda:0,\n",
            "        shape=torch.Size([60, 3]),\n",
            "        data_cls=None),\n",
            "    device=cuda:0,\n",
            "    shape=torch.Size([60]),\n",
            "    data_cls=None)\n",
            "done_spec: Composite(\n",
            "    done: Categorical(\n",
            "        shape=torch.Size([60, 1]),\n",
            "        space=CategoricalBox(n=2),\n",
            "        device=cuda:0,\n",
            "        dtype=torch.bool,\n",
            "        domain=discrete),\n",
            "    terminated: Categorical(\n",
            "        shape=torch.Size([60, 1]),\n",
            "        space=CategoricalBox(n=2),\n",
            "        device=cuda:0,\n",
            "        dtype=torch.bool,\n",
            "        domain=discrete),\n",
            "    device=cuda:0,\n",
            "    shape=torch.Size([60]),\n",
            "    data_cls=None)\n",
            "observation_spec: Composite(\n",
            "    agents: Composite(\n",
            "        observation: UnboundedContinuous(\n",
            "            shape=torch.Size([60, 3, 18]),\n",
            "            space=ContinuousBox(\n",
            "                low=Tensor(shape=torch.Size([60, 3, 18]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
            "                high=Tensor(shape=torch.Size([60, 3, 18]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
            "            device=cuda:0,\n",
            "            dtype=torch.float32,\n",
            "            domain=continuous),\n",
            "        info: Composite(\n",
            "            pos_rew: UnboundedContinuous(\n",
            "                shape=torch.Size([60, 3, 1]),\n",
            "                space=ContinuousBox(\n",
            "                    low=Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
            "                    high=Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
            "                device=cuda:0,\n",
            "                dtype=torch.float32,\n",
            "                domain=continuous),\n",
            "            final_rew: UnboundedContinuous(\n",
            "                shape=torch.Size([60, 3, 1]),\n",
            "                space=ContinuousBox(\n",
            "                    low=Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
            "                    high=Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
            "                device=cuda:0,\n",
            "                dtype=torch.float32,\n",
            "                domain=continuous),\n",
            "            agent_collisions: UnboundedContinuous(\n",
            "                shape=torch.Size([60, 3, 1]),\n",
            "                space=ContinuousBox(\n",
            "                    low=Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
            "                    high=Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
            "                device=cuda:0,\n",
            "                dtype=torch.float32,\n",
            "                domain=continuous),\n",
            "            device=cuda:0,\n",
            "            shape=torch.Size([60, 3]),\n",
            "            data_cls=None),\n",
            "        device=cuda:0,\n",
            "        shape=torch.Size([60, 3]),\n",
            "        data_cls=None),\n",
            "    device=cuda:0,\n",
            "    shape=torch.Size([60]),\n",
            "    data_cls=None)\n"
          ]
        }
      ],
      "source": [
        "print(\"action_spec:\", env.full_action_spec)\n",
        "print(\"reward_spec:\", env.full_reward_spec)\n",
        "print(\"done_spec:\", env.full_done_spec)\n",
        "print(\"observation_spec:\", env.observation_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the commands just shown we can access the domain of each value.\n",
        "Doing this we can see that all specs apart from done have a leading shape ``(num_vmas_envs, n_agents)``.\n",
        "This represents the fact that those values will be present for each agent in each individual environment.\n",
        "The done spec, on the other hand, has leading shape ``num_vmas_envs``, representing that done is shared among\n",
        "agents.\n",
        "\n",
        "TorchRL has a way to keep track of which MARL specs are shared and which are not.\n",
        "In fact, specs that have the additional agent dimension\n",
        "(i.e., they vary for each agent) will be contained in a inner \"agents\" key.\n",
        "\n",
        "As you can see the reward and action spec present the \"agent\" key,\n",
        "meaning that entries in tensordicts belonging to those specs will be nested in an \"agents\" tensordict,\n",
        "grouping all per-agent values.\n",
        "\n",
        "To quickly access the keys for each of these values in tensordicts, we can simply ask the environment for the\n",
        "respective keys, and\n",
        "we will immediately understand which are per-agent and which shared.\n",
        "This info will be useful in order to tell all other TorchRL components where to find each value\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "action_keys: [('agents', 'action')]\n",
            "reward_keys: [('agents', 'reward')]\n",
            "done_keys: ['done', 'terminated']\n"
          ]
        }
      ],
      "source": [
        "print(\"action_keys:\", env.action_keys)\n",
        "print(\"reward_keys:\", env.reward_keys)\n",
        "print(\"done_keys:\", env.done_keys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transforms\n",
        "\n",
        "We can append any TorchRL transform we need to our environment.\n",
        "These will modify its input/output in some desired way.\n",
        "We stress that, in multi-agent contexts, it is paramount to provide explicitly the keys to modify.\n",
        "\n",
        "For example, in this case, we will instantiate a ``RewardSum`` transform which will sum rewards over the episode.\n",
        "We will tell this transform where to find the reward key and where to write the summed episode reward.\n",
        "The transformed environment will inherit\n",
        "the device and meta-data of the wrapped environment, and transform these depending on the sequence\n",
        "of transforms it contains.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = TransformedEnv(\n",
        "    env,\n",
        "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "the :func:`check_env_specs` function runs a small rollout and compares its output against the environment\n",
        "specs. If no error is raised, we can be confident that the specs are properly defined:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92m2025-11-10 22:19:29,061 [torchrl][INFO]\u001b[0m    check_env_specs succeeded!\u001b[92m [END]\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "check_env_specs(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rollout\n",
        "\n",
        "For fun, let's see what a simple random rollout looks like. You can\n",
        "call `env.rollout(n_steps)` and get an overview of what the environment inputs\n",
        "and outputs look like. Actions will automatically be drawn at random from the action spec\n",
        "domain.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rollout of three steps: TensorDict(\n",
            "    fields={\n",
            "        agents: TensorDict(\n",
            "            fields={\n",
            "                action: Tensor(shape=torch.Size([60, 5, 3, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                episode_reward: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                info: TensorDict(\n",
            "                    fields={\n",
            "                        agent_collisions: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        final_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        pos_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
            "                    batch_size=torch.Size([60, 5, 3]),\n",
            "                    device=cuda:0,\n",
            "                    is_shared=True),\n",
            "                observation: Tensor(shape=torch.Size([60, 5, 3, 18]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
            "            batch_size=torch.Size([60, 5, 3]),\n",
            "            device=cuda:0,\n",
            "            is_shared=True),\n",
            "        done: Tensor(shape=torch.Size([60, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "        next: TensorDict(\n",
            "            fields={\n",
            "                agents: TensorDict(\n",
            "                    fields={\n",
            "                        episode_reward: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        info: TensorDict(\n",
            "                            fields={\n",
            "                                agent_collisions: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                                final_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                                pos_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
            "                            batch_size=torch.Size([60, 5, 3]),\n",
            "                            device=cuda:0,\n",
            "                            is_shared=True),\n",
            "                        observation: Tensor(shape=torch.Size([60, 5, 3, 18]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        reward: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
            "                    batch_size=torch.Size([60, 5, 3]),\n",
            "                    device=cuda:0,\n",
            "                    is_shared=True),\n",
            "                done: Tensor(shape=torch.Size([60, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "                terminated: Tensor(shape=torch.Size([60, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
            "            batch_size=torch.Size([60, 5]),\n",
            "            device=cuda:0,\n",
            "            is_shared=True),\n",
            "        terminated: Tensor(shape=torch.Size([60, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
            "    batch_size=torch.Size([60, 5]),\n",
            "    device=cuda:0,\n",
            "    is_shared=True)\n",
            "Shape of the rollout TensorDict: torch.Size([60, 5])\n"
          ]
        }
      ],
      "source": [
        "n_rollout_steps = 5\n",
        "rollout = env.rollout(n_rollout_steps)\n",
        "print(\"rollout of three steps:\", rollout)\n",
        "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that our rollout has ``batch_size`` of ``(num_vmas_envs, n_rollout_steps)``.\n",
        "This means that all the tensors in it will have those leading dimensions.\n",
        "\n",
        "Looking more in depth, we can see that the output tensordict can be divided in the following way:\n",
        "\n",
        "- *In the root* (accessible by running ``rollout.exclude(\"next\")`` ) we will find all the keys that are available\n",
        "  after a reset is called at the first timestep. We can see their evolution through the rollout steps by indexing\n",
        "  the ``n_rollout_steps`` dimension. Among these keys, we will find the ones that are different for each agent\n",
        "  in the ``rollout[\"agents\"]`` tensordict, which will have batch size ``(num_vmas_envs, n_rollout_steps, n_agents)``\n",
        "  signifying that it is storing the additional agent dimension. The ones outside this agent tensordict\n",
        "  will be the shared ones (in this case only done).\n",
        "- *In the next* (accessible by running ``rollout.get(\"next\")`` ). We will find the same structure as the root,\n",
        "  but for keys that are available only after a step.\n",
        "\n",
        "In TorchRL the convention is that done and observations will be present in both root and next (as these are\n",
        "available both at reset time and after a step). Action will only be available in root (as there is no action\n",
        "resulting from a step) and reward will only be available in next (as there is no reward at reset time).\n",
        "This structure follows the one in **Reinforcement Learning: An Introduction (Sutton and Barto)** where root represents data at time $t$ and\n",
        "next represents data at time $t+1$ of a world step.\n",
        "\n",
        "\n",
        "### Render a random rollout\n",
        "\n",
        "If you are on Google Colab, or on a machine with OpenGL and a GUI, you can actually render a random rollout.\n",
        "This will give you an idea of what a random policy will achieve in this task, in order to compare it\n",
        "with the policy you will train yourself!\n",
        "\n",
        "To render a rollout, follow the instructions in the *Render* section at the end of this tutorial\n",
        "and just remove the line ``policy=policy`` from ``env.rollout()`` .\n",
        "\n",
        "\n",
        "## Policy\n",
        "\n",
        "PPO utilises a stochastic policy to handle exploration. This means that our\n",
        "neural network will have to output the parameters of a distribution, rather\n",
        "than a single value corresponding to the action taken.\n",
        "\n",
        "As the data is continuous, we use a Tanh-Normal distribution to respect the\n",
        "action space boundaries. TorchRL provides such distribution, and the only\n",
        "thing we need to care about is to build a neural network that outputs the\n",
        "right number of parameters.\n",
        "\n",
        "In this case, each agent's action will be represented by a 2-dimensional independent normal distribution.\n",
        "For this, our neural network will have to output a mean and a standard deviation for each action.\n",
        "Each agent will thus have ``2 * n_actions_per_agents`` outputs.\n",
        "\n",
        "Another important decision we need to make is whether we want our agents to **share the policy parameters**.\n",
        "On the one hand, sharing parameters means that they will all share the same policy, which will allow them to benefit from\n",
        "each other's experiences. This will also result in faster training.\n",
        "On the other hand, it will make them behaviorally *homogenous*, as they will in fact share the same model.\n",
        "For this example, we will enable sharing as we do not mind the homogeneity and can benefit from the computational\n",
        "speed, but it is important to always think about this decision in your own problems!\n",
        "\n",
        "We design the policy in three steps.\n",
        "\n",
        "**First**: define a neural network ``n_obs_per_agent`` -> ``2 * n_actions_per_agents``\n",
        "\n",
        "For this we use the ``MultiAgentMLP``, a TorchRL module made exactly for\n",
        "multiple agents, with much customization available.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "share_parameters_policy = True\n",
        "\n",
        "policy_net = torch.nn.Sequential(\n",
        "    MultiAgentMLP(\n",
        "        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[\n",
        "            -1\n",
        "        ],  # n_obs_per_agent\n",
        "        n_agent_outputs=2\n",
        "        * env.full_action_spec[env.action_key].shape[-1],  # 2 * n_actions_per_agents\n",
        "        n_agents=env.n_agents,\n",
        "        centralised=False,  # the policies are decentralised (ie each agent will act from its observation)\n",
        "        share_params=share_parameters_policy,\n",
        "        device=device,\n",
        "        depth=2,\n",
        "        num_cells=256,\n",
        "        activation_class=torch.nn.Tanh,\n",
        "    ),\n",
        "    NormalParamExtractor(),  # this will just separate the last dimension into two outputs: a loc and a non-negative scale\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Second**: wrap the neural network in a :class:`TensorDictModule`\n",
        "\n",
        "This is simply a module that will read the ``in_keys`` from a tensordict, feed them to the\n",
        "neural networks, and write the\n",
        "outputs in-place at the ``out_keys``.\n",
        "\n",
        "Note that we use ``(\"agents\", ...)`` keys as these keys are denoting data with the\n",
        "additional ``n_agents`` dimension.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "policy_module = TensorDictModule(\n",
        "    policy_net,\n",
        "    in_keys=[(\"agents\", \"observation\")],\n",
        "    out_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Third**: wrap the :class:`TensorDictModule` in a :class:`ProbabilisticActor`\n",
        "\n",
        "We now need to build a distribution out of the location and scale of our\n",
        "normal distribution. To do so, we instruct the :class:`ProbabilisticActor`\n",
        "class to build a :class:`TanhNormal` out of the location and scale\n",
        "parameters. We also provide the minimum and maximum values of this\n",
        "distribution, which we gather from the environment specs.\n",
        "\n",
        "The name of the ``in_keys`` (and hence the name of the ``out_keys`` from\n",
        "the :class:`TensorDictModule` above) has to end with the\n",
        ":class:`TanhNormal` distribution constructor keyword arguments (loc and scale).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "policy = ProbabilisticActor(\n",
        "    module=policy_module,\n",
        "    spec=env.action_spec_unbatched,\n",
        "    in_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
        "    out_keys=[env.action_key],\n",
        "    distribution_class=TanhNormal,\n",
        "    distribution_kwargs={\n",
        "        \"low\": env.full_action_spec_unbatched[env.action_key].space.low,\n",
        "        \"high\": env.full_action_spec_unbatched[env.action_key].space.high,\n",
        "    },\n",
        "    return_log_prob=True,\n",
        ")  # we'll need the log-prob for the PPO loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Critic network\n",
        "\n",
        "The critic network is a crucial component of the PPO algorithm, even though it\n",
        "isn't used at sampling time. This module will read the observations and\n",
        "return the corresponding value estimates.\n",
        "\n",
        "As before, one should think carefully about the decision of **sharing the critic parameters**.\n",
        "In general, parameter sharing will grant faster training convergence, but there are a few important\n",
        "considerations to be made:\n",
        "\n",
        "- Sharing is not recommended when agents have different reward functions, as the critics will need to learn\n",
        "  to assign different values to the same state (e.g., in mixed cooperative-competitive settings).\n",
        "- In decentralised training settings, sharing cannot be performed without additional infrastructure to\n",
        "  synchronise parameters.\n",
        "\n",
        "In all other cases where the reward function (to be differentiated from the reward) is the same for all agents\n",
        "(as in the current scenario),\n",
        "sharing can provide improved performance. This can come at the cost of homogeneity in the agent strategies.\n",
        "In general, the best way to know which choice is preferable is to quickly experiment both options.\n",
        "\n",
        "Here is also where we have to choose between **MAPPO and IPPO**:\n",
        "\n",
        "- With MAPPO, we will obtain a central critic with full-observability\n",
        "  (i.e., it will take all the concatenated agent observations as input).\n",
        "  We can do this because we are in a simulator\n",
        "  and training is centralised.\n",
        "- With IPPO, we will have a local decentralised critic, just like the policy.\n",
        "\n",
        "In any case, the critic output will have shape ``(..., n_agents, 1)``.\n",
        "If the critic is centralised and shared,\n",
        "all the values along the ``n_agents`` dimension will be identical.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "share_parameters_critic = True\n",
        "mappo = True  # IPPO if False\n",
        "\n",
        "critic_net = MultiAgentMLP(\n",
        "    n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
        "    n_agent_outputs=1,  # 1 value per agent\n",
        "    n_agents=env.n_agents,\n",
        "    centralised=mappo,\n",
        "    share_params=share_parameters_critic,\n",
        "    device=device,\n",
        "    depth=2,\n",
        "    num_cells=256,\n",
        "    activation_class=torch.nn.Tanh,\n",
        ")\n",
        "\n",
        "critic = TensorDictModule(\n",
        "    module=critic_net,\n",
        "    in_keys=[(\"agents\", \"observation\")],\n",
        "    out_keys=[(\"agents\", \"state_value\")],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us try our policy and critic modules. As pointed earlier, the usage of\n",
        ":class:`TensorDictModule` makes it possible to directly read the output\n",
        "of the environment to run these modules, as they know what information to read\n",
        "and where to write it:\n",
        "\n",
        "**From this point on, the multi-agent-specific components have been instantiated, and we will simply use the same\n",
        "components as in single-agent learning. Isn't this fantastic?**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running policy: TensorDict(\n",
            "    fields={\n",
            "        agents: TensorDict(\n",
            "            fields={\n",
            "                action: Tensor(shape=torch.Size([60, 3, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                action_log_prob: Tensor(shape=torch.Size([60, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                episode_reward: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                info: TensorDict(\n",
            "                    fields={\n",
            "                        agent_collisions: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        final_rew: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        pos_rew: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
            "                    batch_size=torch.Size([60, 3]),\n",
            "                    device=cuda:0,\n",
            "                    is_shared=True),\n",
            "                loc: Tensor(shape=torch.Size([60, 3, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                observation: Tensor(shape=torch.Size([60, 3, 18]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                scale: Tensor(shape=torch.Size([60, 3, 2]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
            "            batch_size=torch.Size([60, 3]),\n",
            "            device=cuda:0,\n",
            "            is_shared=True),\n",
            "        done: Tensor(shape=torch.Size([60, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "        terminated: Tensor(shape=torch.Size([60, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
            "    batch_size=torch.Size([60]),\n",
            "    device=cuda:0,\n",
            "    is_shared=True)\n",
            "Running value: TensorDict(\n",
            "    fields={\n",
            "        agents: TensorDict(\n",
            "            fields={\n",
            "                episode_reward: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                info: TensorDict(\n",
            "                    fields={\n",
            "                        agent_collisions: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        final_rew: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        pos_rew: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
            "                    batch_size=torch.Size([60, 3]),\n",
            "                    device=cuda:0,\n",
            "                    is_shared=True),\n",
            "                observation: Tensor(shape=torch.Size([60, 3, 18]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                state_value: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
            "            batch_size=torch.Size([60, 3]),\n",
            "            device=cuda:0,\n",
            "            is_shared=True),\n",
            "        done: Tensor(shape=torch.Size([60, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "        terminated: Tensor(shape=torch.Size([60, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
            "    batch_size=torch.Size([60]),\n",
            "    device=cuda:0,\n",
            "    is_shared=True)\n"
          ]
        }
      ],
      "source": [
        "print(\"Running policy:\", policy(env.reset()))\n",
        "print(\"Running value:\", critic(env.reset()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data collector\n",
        "\n",
        "TorchRL provides a set of data collector classes. Briefly, these\n",
        "classes execute three operations: reset an environment, compute an action\n",
        "using the policy and the latest observation, execute a step in the environment, and repeat\n",
        "the last two steps until the environment signals a stop (or reaches a done\n",
        "state).\n",
        "\n",
        "We will use the simplest possible data collector, which has the same output as an environment rollout,\n",
        "with the only difference that it will auto reset done states until the desired frames are collected.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "collector = SyncDataCollector(\n",
        "    env,\n",
        "    policy,\n",
        "    device=vmas_device,\n",
        "    storing_device=device,\n",
        "    frames_per_batch=frames_per_batch,\n",
        "    total_frames=total_frames,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replay buffer\n",
        "\n",
        "Replay buffers are a common building piece of off-policy RL algorithms.\n",
        "In on-policy contexts, a replay buffer is refilled every time a batch of\n",
        "data is collected, and its data is repeatedly consumed for a certain number\n",
        "of epochs.\n",
        "\n",
        "Using a replay buffer for PPO is not mandatory and we could simply\n",
        "use the collected data online, but using these classes\n",
        "makes it easy for us to build the inner training loop in a reproducible way.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "replay_buffer = ReplayBuffer(\n",
        "    storage=LazyTensorStorage(\n",
        "        frames_per_batch, device=device\n",
        "    ),  # We store the frames_per_batch collected at each iteration\n",
        "    sampler=SamplerWithoutReplacement(),\n",
        "    batch_size=minibatch_size,  # We will sample minibatches of this size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss function\n",
        "\n",
        "The PPO loss can be directly imported from TorchRL for convenience using the\n",
        ":class:`~.objectives.ClipPPOLoss` class. This is the easiest way of utilising PPO:\n",
        "it hides away the mathematical operations of PPO and the control flow that\n",
        "goes with it.\n",
        "\n",
        "PPO requires some \"advantage estimation\" to be computed. In short, an advantage\n",
        "is a value that reflects an expectancy over the return value while dealing with\n",
        "the bias / variance tradeoff.\n",
        "To compute the advantage, one just needs to (1) build the advantage module, which\n",
        "utilises our value operator, and (2) pass each batch of data through it before each\n",
        "epoch.\n",
        "The GAE module will update the input :class:`TensorDict` with new ``\"advantage\"`` and\n",
        "``\"value_target\"`` entries.\n",
        "The ``\"value_target\"`` is a gradient-free tensor that represents the empirical\n",
        "value that the value network should represent with the input observation.\n",
        "Both of these will be used by :class:`ClipPPOLoss` to\n",
        "return the policy and value losses.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\winds\\Desktop\\ML\\RL\\RL-Georgia Tech\\.venv\\Lib\\site-packages\\torchrl\\objectives\\ppo.py:511: DeprecationWarning: 'entropy_coef' is deprecated and will be removed in torchrl v0.11. Please use 'entropy_coeff' instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "loss_module = ClipPPOLoss(\n",
        "    actor_network=policy,\n",
        "    critic_network=critic,\n",
        "    clip_epsilon=clip_epsilon,\n",
        "    entropy_coef=entropy_eps,\n",
        "    normalize_advantage=False,  # Important to avoid normalizing across the agent dimension\n",
        ")\n",
        "loss_module.set_keys(  # We have to tell the loss where to find the keys\n",
        "    reward=env.reward_key,\n",
        "    action=env.action_key,\n",
        "    value=(\"agents\", \"state_value\"),\n",
        "    # These last 2 keys will be expanded to match the reward shape\n",
        "    done=(\"agents\", \"done\"),\n",
        "    terminated=(\"agents\", \"terminated\"),\n",
        ")\n",
        "\n",
        "\n",
        "loss_module.make_value_estimator(\n",
        "    ValueEstimators.GAE, gamma=gamma, lmbda=lmbda\n",
        ")  # We build GAE\n",
        "GAE = loss_module.value_estimator\n",
        "\n",
        "optim = torch.optim.Adam(loss_module.parameters(), lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop\n",
        "We now have all the pieces needed to code our training loop.\n",
        "The steps include:\n",
        "\n",
        "* Collect data\n",
        "    * Compute advantage\n",
        "        * Loop over epochs\n",
        "            * Loop over minibatches to compute loss values\n",
        "                * Back propagate\n",
        "                * Optimise\n",
        "            * Repeat\n",
        "        * Repeat\n",
        "    * Repeat\n",
        "* Repeat\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "episode_reward_mean = 1.7298864126205444: 100%|| 5/5 [00:20<00:00,  3.98s/it] "
          ]
        }
      ],
      "source": [
        "pbar = tqdm(total=n_iters, desc=\"episode_reward_mean = 0\")\n",
        "\n",
        "episode_reward_mean_list = []\n",
        "for tensordict_data in collector:\n",
        "    tensordict_data.set(\n",
        "        (\"next\", \"agents\", \"done\"),\n",
        "        tensordict_data.get((\"next\", \"done\"))\n",
        "        .unsqueeze(-1)\n",
        "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
        "    )\n",
        "    tensordict_data.set(\n",
        "        (\"next\", \"agents\", \"terminated\"),\n",
        "        tensordict_data.get((\"next\", \"terminated\"))\n",
        "        .unsqueeze(-1)\n",
        "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
        "    )\n",
        "    # We need to expand the done and terminated to match the reward shape (this is expected by the value estimator)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        GAE(\n",
        "            tensordict_data,\n",
        "            params=loss_module.critic_network_params,\n",
        "            target_params=loss_module.target_critic_network_params,\n",
        "        )  # Compute GAE and add it to the data\n",
        "\n",
        "    data_view = tensordict_data.reshape(-1)  # Flatten the batch size to shuffle data\n",
        "    replay_buffer.extend(data_view)\n",
        "\n",
        "    for _ in range(num_epochs):\n",
        "        for _ in range(frames_per_batch // minibatch_size):\n",
        "            subdata = replay_buffer.sample()\n",
        "            loss_vals = loss_module(subdata)\n",
        "\n",
        "            loss_value = (\n",
        "                loss_vals[\"loss_objective\"]\n",
        "                + loss_vals[\"loss_critic\"]\n",
        "                + loss_vals[\"loss_entropy\"]\n",
        "            )\n",
        "\n",
        "            loss_value.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                loss_module.parameters(), max_grad_norm\n",
        "            )  # Optional\n",
        "\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "\n",
        "    collector.update_policy_weights_()\n",
        "\n",
        "    # Logging\n",
        "    done = tensordict_data.get((\"next\", \"agents\", \"done\"))\n",
        "    episode_reward_mean = (\n",
        "        tensordict_data.get((\"next\", \"agents\", \"episode_reward\"))[done].mean().item()\n",
        "    )\n",
        "    episode_reward_mean_list.append(episode_reward_mean)\n",
        "    pbar.set_description(f\"episode_reward_mean = {episode_reward_mean}\", refresh=False)\n",
        "    pbar.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n",
        "\n",
        "Let's plot the mean reward obtained per episode\n",
        "\n",
        "To make training last longer, increase the ``n_iters`` hyperparameter.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAHHCAYAAABTMjf2AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUHBJREFUeJzt3QlcVFX/x/EfO6ICriiK+76CuOSSWlpWVprlbu71VFambfqvJ7PNp1UtTe2xtNJyybJ6UnPLzCWVRcU1FxREERUFBFln/q9zbCYwUSBg5s583q/X5MzlMpx7bzhfz/mdc13MZrNZAAAAnJCrrRsAAABgKwQhAADgtAhCAADAaRGEAACA0yIIAQAAp0UQAgAATosgBAAAnBZBCAAAOC2CEAAAcFoEIcDJvPrqq+Li4lKqP/PEiRP6Zy5cuLBUf67RqHOkrg+A0kMQAuyYCg7qwzG/x++//27rJgKAobnbugEAbu61116TunXr/m17gwYNCn36Xn75ZZk0aRKnHQAIQoAx3H333dK2bdtieS93d3f9cDTZ2dliMpnE09NT7FVqaqqULVvW1s0AkAtDY4ADsNTgvPfeezJ9+nSpXbu2lClTRrp16yb79u27aY3QunXrpEuXLuLv7y/lypWTxo0by//93//l2SchIUHGjBkjAQEB4u3tLa1bt5bPP//8b225dOmSjBw5Uvz8/PT7jRgxQm+7nkOHDslDDz0kFStW1O+pwt4PP/xQqOOdMWOG1K9fX7y8vOTAgQMFel/VHjc3N/nwww+t286fPy+urq5SqVIlMZvN1u2PP/64VKtWzfr6t99+k/79+0utWrX0zwwKCpIJEybIlStX8rRRnQN1Lo8dOyb33HOPlC9fXoYOHaq/lpGRob+nSpUqevv9998vp06dkoLYtGmTPvZly5bJ1KlTpUaNGvo91PEmJSXp937mmWekatWq+uePGjVKb7vWokWLJDQ0VP9/os7ToEGDJDY2Ns8+hT3WuLg46du3r36uju25556TnJycAh0XYCuO989CwAGpDzj1QZ2b+jBUH9q5ffHFF5KSkiLjxo2T9PR0mTlzptx+++0SFRWlA8z17N+/X+69915p1aqVHoJTH3hHjx6VrVu3WvdRH3zdu3fX25988kk9TLd8+XL9AahCxfjx4/V+KkD06dNHtmzZIo899pg0bdpUvvvuOx2GrvdzO3furD/I1VCd6ilRH+7qg3TFihXywAMP3PS8LFiwQB/no48+qtutPtAL8r4qoLVo0UI2b94sTz/9tH4v1WZ1ThMTE3Wgat68uTUM3HrrrdafqY47LS1NByR1/nfu3CkfffSRDjLqa9f2UvXq1UuHTBXafHx89PaxY8fqIDJkyBDp1KmTbNy4UXr37i2FMW3aNB1i1DGq66La4OHhocPcxYsXdeBVNWSqzkxdr1deecX6vW+++ab8+9//lgEDBui2nDt3Tn9/165dJTIyUp+fwh6rCjzqWDt06KCPdf369fL+++/rkKq+H7BbZgB2a8GCBapr4roPLy8v637R0dF6W5kyZcynTp2ybt+xY4fePmHCBOu2KVOm6G0W06dP16/PnTuXbztmzJih91m0aJF1W2Zmprljx47mcuXKmZOTk/W2lStX6v3eeecd637Z2dnmW2+9VW9Xx2PRo0cPc8uWLc3p6enWbSaTydypUydzw4YNb3heLMfr6+trTkhIyPO1gr7vuHHjzAEBAdbXEydONHft2tVctWpV85w5c/S2CxcumF1cXMwzZ8607peWlva39kybNk3vd/LkSeu2ESNG6DZOmjQpz767d+/W25944ok824cMGaK3q+tzI7/88over0WLFvoaWAwePFi34e67786zv7pGtWvXtr4+ceKE2c3Nzfzmm2/m2S8qKsrs7u6eZ3thj/W1117Ls29ISIg5NDT0hscD2BpDY4ABzJ49Ww9f5X6sXr36b/upXg/VE2LRvn17/S/0VatW5fveln/9f//997rG5nrU96vhocGDB1u3qd4H1Zty+fJl+fXXX637qfqj3D0AagjqqaeeyvN+qtdF9YKoHgnVg6V6u9TjwoULulfhyJEjepjlZh588EE9BFOU91W9PGfPnpXDhw9be35Uj4jarp5beolUL1fuHiHVC5O75ke9v+rVUfup3pRrXdsbYrkWlp4oCzWcVRjDhw/X18BCXWfVhtGjR+fZT21XQ16qd0r59ttv9XVW58hyftRDXd+GDRvKL7/8UuRjVb2Auanzdvz48UIdF1DaGBoDDEAFmoIUS6sPsms1atRIDw3lZ+DAgTJ//nw9RKKGWXr06CH9+vXTNSdqmEU5efKkfm/Laws19GX5uuXP6tWr6xqR3FTNUW5qKEd9mKrhGfW4HlWTlDvUXc+1M+kK876WcKNCT82aNfUH+xtvvKGDlRrasXzN19dX10NZxMTE6GEmVXOkhqCuHcLMTYVC9d65qXOkzqMaMrrROboZVbeTm6rJUlQdz7XbVfBRbVPDWyoMqnN0vf9XlNzhqjDHqmqxcodSpUKFCn/7PsDeEIQAJ6f+1a9qZVRPwE8//SRr1qyRpUuX6tqitWvX6h6d4mbpeVLFtKqn5noKsjRA7h6Lwr5vYGCgDlLq2OvUqaPDQceOHfWHuap5UoFFBSHVA2IJgKoO5o477tA9Ty+++KI0adJE1yCpXiZVL3Vtj5qqW7o2PBaX/K5LftstBeCqjaoWSvUoXm9fS4gt7LGWxP8nQGkgCAEORP1r/1p//PGH/qC/EfVhrXqC1OODDz6Qt956S1566SUdjnr27Klnoe3du1d/+OX+YFezsxT1dcufGzZs0MNluXuFLMNPFvXq1bP2Pqj3Ly6FfV/VK6SCkApEwcHBevaV6v1RvSgqEEZEROiZWRaq6FydTzVbTg1NWaihyoJS50idRzWbLHcv0LXnqKSonigVitQxq97C/BTHsQJGQI0Q4EBWrlyZp7ZGzfLZsWOHXocoP+pf/NdSoUCxTLtW07/j4+N1T5GFqjlRM4hU4FHT9C37qe1z5syx7qd6FtR+uamp3WoW2rx58+TMmTN/+/lqFlNRFPZ9VRBSU/HVcVmGylTQU71AKhBmZWXlqQ+y9Hrknl6vnqvZeQVluRa5p+4rahmA0qCGPdVxqICX+zgU9VrVUxXXsQJGQI8QYABqGMPS+5Kb+sC29IJYhn3UVG1VoKtCjPpwVXUhL7zwQr7vrabMq14RNX1b9VaoGpqPP/5Y17ao91LU9HQVLtSQSHh4uO5h+uabb/QUe/UzVE+Kct999+mp66rWSAWMZs2a6eLca+tJLAXg6v1btmwpjzzyiD4OVby8fft2PT17z549RTpXhXlfS8hRvTGqF8xCFU2rc66Gttq1a2fdroaHVI+KGnpTgVPVD6kp+YWpg1EhUxWdq3Oszou6hqoXTdU3lQbVflULNXnyZH2NVIG9un7R0dF6qQN1rdXxFcexAkZAEAIMIPcaMNeuo5M7CKkhDNWjocKJCjSqyHrWrFm6gDk/ajE/9YH42Wef6VlBlStX1j08qsfAUoCranHUQn4q4KihkuTkZD2so36+CkcW6merwlo1A0qtk6NqUdT7q/VkQkJC8vxcFZLCwsL0z1Fr3aieCNWjo/bL73gLojDvq45BfU2dK0voyx2Q1PlTYchCDbn9+OOPesaXWsdHFQirdYnU2kq5C6pvRp1rVYu0ePFi3Yun6rFUfda1hc4lRV1HNSymFt+0DP2pn33nnXfq61WcxwrYOxc1h97WjQDwz6ggo2o+3n33Xf0veABAwVAjBAAAnBZBCAAAOC2CEAAAcFrUCAEAAKdFjxAAAHBaBCEAAOC0WEfoJtRS+KdPn9YLjqk1UQAAgP1TqwOlpKTo+wre6J5/BKGbUCGotBY5AwAAxSs2NlavlJ8fgtBNWG4doE6kWmIeAADYP7UCvurIsHyO54cgdBOW4TAVgghCAAAYy83KWiiWBgAATosgBAAAnBZBCAAAOC2CEAAAcFoEIQAA4LQIQgAAwGkRhAAAgNMiCAEAAKdFEAIAAE6LIAQAAJwWQQgAADgtghAAAHBaBCEAAGATJpNZthw5Lzkms20aQBACAAClLT0rR77eGSN3ztgswz7dIRsOnhVbcbfZTwYAAE7lXEqGfPn7SVn0+0lJTM3U28p5uUtCSobN2kQQAgAAJepwfIp8uuW4rIw8LZk5Jr2thn8ZGdW5jgxsFyTlvT3EVghCAACg2JnNZvntyHmZvyVaNv9xzro9OMhfHrm1nvRqHiDubrYvVSYIAQCAYq3/+WH3aZm/5bj8cfay3ubqInJXi2oypks9Ca1dQewJQQgAAPxj5y9n6Nof9Th/+Wr9T1lPNxnYrpYeAguq6CP2iCAEAACK7MjZFPlsa7SsiIiTzOyr9T+Bft4yqnNdGdg+SHxtWP9TEAQhAABQ6PqfrUcv6OGvTYf/qv9pXdNPxt5aTw+DedhB/U9BEIQAAECBZGRfrf/5dEu0HIpP0dtcXETubBagC6BV/Y+L2mAgBCEAAHBDas2fxb+flM+3q/qfq2v++Hi6yYC2Qbr+p3alsmJUBCEAAHBdRxMuX63/CT8lGX/W/1T385aRnerIoHa1xM/Hvut/CoIgBAAA8tT/bD+m6n+iZeOhBOv2ljVU/U9duadldcPU/xQEQQgAAIia8fXjHrX+T7QcPJOsz4gq97mjaYAugG5Xx3j1PwVBEAIAwIldTM2Ur3bGyOfbTljv+VXGw036t60pozvXlTqVjVv/UxAEIQAAnNDxc1frf74JPyXpWVfrfwJ8vWREpzoypH0t8ffxFGdAEAIAwInqf34/nqhvgLr+4F/1P80DfXX9T++WgeLp7jj1PwVBEAIAwAnqf36KOi3zf4uW/aev1v8oPZtW1ff/uqVeRYes/ykIghAAAA7qUtpf9T9nk6/W/3h7uMpDoVfrf+pVKSfOjiAEAICDiT6fKgu2RsvysFNyJStHb6tS3kuv/6PqfyqUdY76n4IgCAEA4CD1PzujE/X09/UHz4rZfHV70+q+MrZLXbm3dXXxcnezdTPtDkEIAAADy8oxyaqoM7r+Jyouybr99iZVdQDqWL+S09b/FARBCAAAA0pKy5Kvd12t/zmTlK63ebm7yoN/1v80qEr9T0EQhAAAMJCTF1T9zwlZFhYraZlX638ql/OSER1ry9BbaktF6n8KhSAEAIAB6n/CTl6U+b8dl7UH/qr/aVKtvIzpUlfuDw6k/qeICEIAANipbFX/sy9ePv3tuOw59Vf9T/fGVWRsl3rSuQH1P/+UoZaP3Lx5s9x3330SGBioC79Wrlx5w/03bdqk97v2ER8fX2ptBgCgsJLTs+S/m49Lt3c3ydNfR+oQpFZ8Htw+SNZN6CoLR7WXLg0rUwTtbD1Cqamp0rp1axk9erT069evwN93+PBh8fX1tb6uWrVqCbUQAICii01M0/U/S3fFSKq1/sdTHr6ljgy9pZauBYITB6G7775bPwpLBR9/f/8SaRMAAP9U+MmL+v5fa/bFi+nP+p9GAeX08Jeq//H2YP2fkmKoIFRUwcHBkpGRIS1atJBXX31VOnfunO++aj/1sEhO/uueLAAAFGf9z5r98Xr9n92xl6zbuzZS9T915VaGvkqFQweh6tWry9y5c6Vt27Y63MyfP1+6d+8uO3bskDZt2lz3e6ZNmyZTp04t9bYCAJxDSnqWLN0Vq4fA4i5d0ds83VzlgZAaMrpLXWlcrbytm+hUXMxqTp4BqaLn7777Tvr27Vuo7+vWrZvUqlVLvvzyywL3CAUFBUlSUlKeOiMAAArj1MU0Wbj1hCzZFSuXM7L1NrXmz8O31JZht9TW9wJD8VGf335+fjf9/HboHqHrad++vWzZsiXfr3t5eekHAADFITLmor7/1+qoM9b6H7Xqs1r/R/UCUf9jW04XhHbv3q2HzAAAKCk5JrOsVfU/W6J1IbRFlwaVZcytdaVbwyri6sr9v+yBoYLQ5cuX5ejRo9bX0dHROthUrFhRD3dNnjxZ4uLi5IsvvtBfnzFjhtStW1eaN28u6enpukZo48aNsnbtWhseBQDAUakhr2Wq/mdbtMQm/lX/0yc4UNf/qDvBw74YKgiFhYXJbbfdZn09ceJE/eeIESNk4cKFcubMGYmJibF+PTMzU5599lkdjnx8fKRVq1ayfv36PO8BAMA/pYqe1c1Pv94RIyl/1v9U8PG4Wv/TsbZULe/NSbZThi2WtrdiKwCA89kTe0kPf62KOqOHw5R6Vcrq+p9+ITWljCfr/9gKxdIAAJQAFXjWHTirF0DcdeKv+p9O9SvJ2FvrSvdGVan/MRBDDY0BAGArqRnZsjwsVj7bekJiEtP0Ng83F7mvdaDuAWoe6MfFMSCCEAAAN3Am6Yos3HZCvlL1P+lX63/8fTxkaIdaMrxjHQnwpf7HyAhCAABcR9SpJJm/5bj8tPeMZP9Z/1O3clk9++vBNjXEx5OPUEfAVQQAIFf9z4aDZ3UB9M7oROt5uaVeRX0D1NubUP/jaAhCAACnl5aZLd+En5LPtkTLiQtX63/cXf+q/2lRg/ofR0UQAgA4rfikdPl8+9X6n6QrWXqbr7e7DL2ltozoWEeq+VH/4+gIQgAAp7MvLkk+3RItP+45ba3/qV3JR/f+PNimppT14uPRWXClAQBOwWQyy8ZDCboA+vfjf9X/tK+r6n/qSo+mAeLG/b+cDkEIAODQrmTmyDcRp2TBlmg5fj5Vb1OB595W1XUPUKua/rZuImyIIAQAcEhnk9Pli+0nZPGOGLmUdrX+p7y3uwzpUEvX/wT6l7F1E2EHCEIAAIdy4HSyHv5S9T9ZOVfrf2pV9JHRnetI/7ZB1P8gD4IQAMAh6n9+/eOcDkBbj16wbm9Xp4KM6VJP7mhG/Q+ujyAEADCs9Kwc+TYiTt8A9di5v+p/7ml5tf4nOIj6H9wYQQgAYDiH41NkWVisfBtxSi5a6n+83GWwqv/pVEdqUP+DAiIIAQAMITk9S9f9LAs7JXtiL1m316xQRkZ3risD2gVJOdb/QSERhAAAdstsNus1f5aHxcqqfWckPctkvf1Fj6ZVZUDbIOnWqIq4u7nauqkwKIIQAMDunEm6IivCT8ny8FNy8s97fykNqpaTgW2D5IE2NaRyOS+bthGOgSAEALALmdkmWX/wrK792fzHOfnzzhd6uOu+1tV1748qfnZxcbF1U+FACEIAAJs6FJ8sy3adkpW74yQxNTPPrS9U+LmnZTXx8eTjCiWD/7MAADYpfP5h92ld+7PnVJJ1e9XyXvJQaE298GHdymW5MihxBCEAQKktevh79AVZHnZKVkWdkYzsvwqfezYNkAHtakrXhhQ+o3QRhAAAJV74/E3Y1cLnmMS/Cp8bqsLndkHSN4TCZ9gOQQgAUOwysnNkw8EEWborVn47cm3hc6AMaFuTwmfYBYIQAKBYC59V+FkZGWdd8Vnp8Gfh890UPsPOEIQAAP9I0hXLis+xsjdX4XOA75+Fz6FBUofCZ9gpghAAoMiFz8t2xcrqffHWwmcPtz8Ln9sGya0NK7PiM+weQQgAUGCnL/214nPuwudGAeV0+HkgpIZUYsVnGAhBCABw08Ln9QcSZGnY1cJn85+Fz+pu7/cFq8LnIGld048Vn2FIBCEAwHUdPJOs636uV/ispr3f3aK6lPF04+zB0AhCAIA8hc8/qMLnXbESFUfhMxwfQQgAnJwufD5+Qff+5Ff43LVRFXFz5WancDwEIQBwUnHWwudYiU28Yt1O4TOcCUEIAJys8HndgbOyLOwUhc8AQQgAnMOB038WPu+Ok0u5Cp9vqffnis8UPsNJ0SMEAA4qKU0VPsfp3p/chc/VfL31is/qwYrPcHYEIQBwsMLn7X8WPq+5pvD5jmYB0l8VPjek8BmwIAgBgIMUPn8TdrXw+dTFvwqfGweUlwHtgqRvcCArPgPXQRACAAMXPq/drwqfY2XL0fN5Vny+/88Vn1ux4jNwQwQhADCY/aeTZHnYqb8VPnesV0kGtKspdzVnxWegoAhCAGCQwufvdeFzrOyLS7Zur+73V+Fz7UplbdpGwIgIQgBgx4XP2479Wfi8P14ycxU+39msmvRvW1NupfAZ+EcIQgBgZ05dTJNv1IrPYad0EbRFk2rldd1P35AaUrGsp03bCDgKghAA2IH0rBxZe+CsLL+28NnbXfr8WfjcsoafuLhwvy+gOBGEAMDGhc/qTu8rd5/Wd37PXfg8sF2Q9GpeTcp4unGNgBJCEAIAGxQ+qxlfqvZn/+m8hc/9deFzkNSq5MN1AUoBQQgASrHweWlYrPx8ncJntehhlwaVxc2VoS+gNBGEAKCEC59V0bMqfr628FkNffUJpvAZsCWCEACUUOGzqv3Zeuzvhc8D29aSFjV8KXwG7ABBCACKyb64JF33szIyTpLTs63bO9X/q/DZ24PCZ8CeEIQA4B+4lJYp3+8+LUt3xcqBM38VPgf+ueKzutt7UEUKnwF75SoGsnnzZrnvvvskMDBQdymvXLnypt+zadMmadOmjXh5eUmDBg1k4cKFpdJWAI5d+PzbkXPy5FcR0v7NDTLlh/06BHm6uUrvVtXli9Ht5bcXb5eJdzYmBAF2zlA9QqmpqdK6dWsZPXq09OvX76b7R0dHS+/eveWxxx6TxYsXy4YNG2Ts2LFSvXp16dWrV6m0GYDjiE28uuJzfoXPfYNrSAVWfAYMxVBB6O6779aPgpo7d67UrVtX3n//ff26adOmsmXLFpk+fTpBCECBC5/VdHdV+7P16AXrdlX4rIKPCkDNAyl8BozKUEGosLZv3y49e/bMs031BD3zzDP5fk9GRoZ+WCQn/zXmD8B55Ff43LlBJX27CwqfAcfg0EEoPj5eAgIC8mxTr1W4uXLlipQpU+Zv3zNt2jSZOnVqKbYSgL24mKoKn9WKz6f+XvjcNkiv+kzhM+BYHDoIFcXkyZNl4sSJ1tcqNAUFBdm0TQBKTo7JLFuPnte9P2v3n5XMnKsrPqvC5zubB+jen86s+Aw4LIcOQtWqVZOzZ8/m2aZe+/r6Xrc3SFGzy9QDgOMHoHmbj8ni32PyFD43re4rA9vW1Cs+U/gMOD6HDkIdO3aUVatW5dm2bt06vR2A80rNyJanv46UDYcS9GtfVfgcUkP3/lD4DDgXQwWhy5cvy9GjR/NMj9+9e7dUrFhRatWqpYe14uLi5IsvvtBfV9PmZ82aJS+88IKecr9x40ZZtmyZ/PTTTzY8CgC2FJ+ULmM+36Xv+u7l7ipT7msu/drUYMVnwEkZKgiFhYXJbbfdZn1tqeUZMWKEXijxzJkzEhMTY/26mjqvQs+ECRNk5syZUrNmTZk/fz5T5wEndeB0soxeuEvik9OlUllP+e+IttKmVgVbNwuADbmYzZbbAeJ6VLG0n5+fJCUl6doiAMb0y6EEvRJ0amaO1K9SVhaOas8MMMCBFfTz21A9QgBQFF9uP6Fvg2EyX70B6pyhoeLn48HJBEAQAuDYM8PeWnVQPt0SrV+rm6C+9UBL8XQ31G0WAZQgeoQAOKS0zGwZv2S3rDtwdQmN53s1lie619c3bAYAC4IQAIeTkKxmhoVJVFyS7v15r39rub91oK2bBcAOEYQAOJRD8ckyesEuOZ2ULhV8POS/w9tK2zoVbd0sAHaKIATAYfz6xzkZtzhCLmdkS73KZeWzke2kTuWytm4WADtGEALgEL7aESP//n6fLpBuX7eifPJwqPj7eNq6WQDsHEEIgKGZTGZ5e80hmbf5uH7dL6SGTHuwpXi5u9m6aQAMgCAEwLCuZObIxGW7ZfW+eP16Qs9G8nSPBswMA1BgBCEAhnQuJUPGfhEme2Iviaebq7z9UEt5IKSmrZsFwGAIQgAM58jZFBm5YJfEXboi/j4eMm9YqHSoV8nWzQJgQAQhAIay9eh5eWxRuKSkZ0udSj56Zli9KuVs3SwABkUQAmAYy3bFyv99FyXZJrO0rV1BPhneViqWZWYYgKIjCAEwxMyw99Yelo83HdOv1SrR7zzUSrw9mBkG4J8hCAGwa+lZOfLs8j3y094z+vXTtzeQCXc0YmYYgGJBEAJgty5czpBHvgiTiJhL4uHmItP6tdJ3kAeA4kIQAmCXjiZcltELd0lMYpr4ervL3IdDpVP9yrZuFgAHQxACYHe2H7sg//oyTJLTsyWoYhlZMLK9NKjKzDAAxY8gBMCufBN+SiZ/u1eycszSppa/vnt8pXJetm4WAAdFEAJgF8xms0xf94d8uPGoft27VXV5v39rZoYBKFEEIQA2l5GdIy98s1e+331av36ie3157s7G4urqYuumAXBwBCEANpWYmqnrgXaduCjuri7y5gMtZGC7WlwVAKWCIATAZqLPp8qoBTvlxIU0Ke/lLnOGhUqXhswMA1B6CEIAbGJndKI8+mWYXErLkhr+ZWTBqHbSKKA8VwNAqSIIASh1KyPjdE1QZo5JWgf5y/zhbaVKeWaGASh9BCEApToz7MMNR2X6+j/067uaV5PpA4OljCf3DANgGwQhAKU2M2zyiij5NjJOv/5X13ry4l1NmBkGwKYIQgBK3KU0NTMsXHZEJ4qbq4u81qe5DO1QmzMPwOYIQgBK1MkLambYLjl+PlXKebnL7KFtpFujKpx1AHaBIASgxISfTJRHvgjXawUF+nnLZ6PaSZNqvpxxAHaDIASgRPy457Q8u3yPZGabpGUNP/l0RFup6uvN2QZgVwhCAIp9ZtjHm47Juz8f1q/vaBYgMwcFi48nf90AsD/8zQSg2Kjen5e+i5Ll4af06zFd6sr/3dNUF0gDgD0iCAEoFklXsuTxReGy7dgFUbnn1fuby/COdTi7AOwaQQjAPxabmCajFu6SowmXpaynm8wa0kZua1KVMwvA7hGEAPwjETEX5ZHPw+RCaqZU8/WWT0e2leaBfpxVAIZAEAJQZKuizsiEpbslI9skzar7ymcj20k1P2aGATAOghCAIs0Mm7f5uPxn9SH9+vYmVeWjwSFS1ou/UgAYC39rASiUrByTvPL9Pvl6Z6x+PbJTHfn3vc2YGQbAkAhCAAosOT1Lxi2OkN+OnNczw1QAGtW5LmcQgGERhAAUyKmLaTJ64S754+xlKePhpofCejYL4OwBMDSCEICb2hN7ScZ8HibnL2dI1fJe8umIdtKyJjPDABgfQQjADa3ZFy/PLI2U9CyTNKlWXs8MC/Qvw1kD4BAIQgDynRn26ZZoeXPVQTGbRbo1qiKzhoRIeW8PzhgAh0EQAvA32TkmefXH/bLo9xj9emiHWjL1/ubi7ubK2QLgUAhCAPJISc+SJ7+KlF//OCcuLiIv3dNU3zzVRb0AAAdDEAJgdfrSFT0z7FB8inh7uMqMgSFyV4tqnCEADosgBEDbF5ekQ1BCSoZULqdmhrWV1kH+nB0ADo0gBEDWHzgrTy+JlLTMHGkUUE7PDKtZwYczA8DhFTgITZw4scBv+sEHHxS1PQBK2YKt0fL6/w6IySxya8PKMntoG/FlZhgAJ1HgIBQZGZnndUREhGRnZ0vjxo316z/++EPc3NwkNDRUStLs2bPl3Xfflfj4eGndurV89NFH0r59++vuu3DhQhk1alSebV5eXpKenl6ibQSMIMdk1gFo4bYT+vXg9kHyWp8W4sHMMABOpMBB6JdffsnT41O+fHn5/PPPpUKFCnrbxYsXdei49dZbS6alIrJ06VLdMzV37lzp0KGDzJgxQ3r16iWHDx+WqlWrXvd7fH199dctmPkCiKRmZMvTX0fKhkMJ+nRMuruJ/KtrPX4/ADgdF7NaNa2QatSoIWvXrpXmzZvn2b5v3z6588475fTp01ISVPhp166dzJo1S782mUwSFBQkTz31lEyaNOm6PULPPPOMXLp0qcg/Mzk5Wfz8/CQpKUmHKsDo4pPSZcznu2T/6WTxcneV6QOD5Z6W1W3dLAAoVgX9/HYt6pufO3fub9vVtpSUFCkJmZmZEh4eLj179rRuc3V11a+3b9+e7/ddvnxZateurQNTnz59ZP/+/Tf8ORkZGfr4cj8AR3HgdLL0nb1Vh6BKZT3l60dvIQQBcGpFCkIPPPCAHgb79ttv5dSpU/qxYsUKGTNmjPTr16/4Wyki58+fl5ycHAkIyHu3a/Va1Qtdj6pf+uyzz+T777+XRYsW6R6kTp066fbmZ9q0aTpBWh4qQAGO4JdDCdJ/7jaJT06X+lXKyspxnaVNratD2wDgrIo0fV7V6Dz33HMyZMgQycrKuvpG7u46CKlCZnvRsWNH/bBQIahp06Yyb948ef3116/7PZMnT84zQ071CBGGYHRfbj8hU37Yr2eGdapfSeYMDRU/H+4ZBgCFDkKqVyYsLEzefPNNHXqOHTumt9evX1/Kli1bYme0cuXKelba2bNn82xXr6tVK9jKtx4eHhISEiJHjx7Ndx81q0w9AEeZGfbWqoP65qlK/9Ca8uYDLcXTnXuGAYBS6L8NVRhRBdGqAFkFn1atWulHSYYgxdPTU0/N37Bhg3WbGupSr3P3+twsxEVFRUn16hSGwvGlZWbLY4vCrSHo+V6N5Z2HWhGCAOCfDo21aNFCjh8/LnXr1pXSpIasRowYIW3bttVrB6np86mpqda1goYPH65ntKk6H+W1116TW265RRo0aKCDm+rBOnnypIwdO7ZU2w2UtoRkNTMsTKLiknTweb9/a7mvdSAXAgCKIwi98cYbukZI1dmoXppre4NKapr5wIED9cy0V155RRdIBwcHy5o1a6wF1DExMXommYVa2+iRRx7R+6r1jlRbt23bJs2aNSuR9gH24FB8soxesEtOJ6VLxbKe8t/hoRJau6KtmwUAjrOOUO6wkXuBQvVW6rUagnIUrCMEI/n1j3MybnGEXM7IlnqVy8qCUe2kdqWSHbYGACN/fhepRyj3KtMA7MNXO2Lk39/v0wXS7etWlE8eDhV/H09bNwsA7FqRglC3bt2KvyUAisRkMsvbaw7JvM3H9et+ITVk2oMtxcvdjTMKACURhCzS0tJ0XY5a9Tk3NYsMQMm7kpkjE5ftltX7ri4qOqFnI3m6RwPuGQYAJRmEVMGymqm1evXq637dkWqEAHt1LiVDxn4RJntiL4mnm6u8/VBLeSCkpq2bBQCGUqRV1Sw3Mt2xY4eUKVNGz9xSd6Jv2LCh/PDDD8XfSgB5HDmbou8ZpkKQv4+HfDmmPSEIAEqrR2jjxo36/l1qPR81g0zd1PSOO+7QVdlqDZ/evXsX5W0BFMDWo+f1Qokp6dlSp5KPfDayndSrUo5zBwCl1SOkFjGsWrWqfq7W57Hcib5ly5YSERFRlLcEUADLdsXKiM926hDUrk4F+faJzoQgACjtIKTu6n748GH9vHXr1vompnFxcfpmrNy+AiiZmWHvrDkkL6zYK9kms/QJDpRFYzvoBRMBAKU8NDZ+/Hg5c+aMfj5lyhS56667ZPHixfp+YAsXLvwHzQFwrfSsHHl2+R75ae/V37mnb28gE+5oxMwwALDVytLXm0Z/6NAhqVWrlr5LvCNhZWnY0oXLGfLIF2ESEXNJPNxcZFq/VvJQKDPDAMCmK0urG67Wq1fP+trHx0fatGlTlLcCkI+jCZdl9MJdEpOYJr7e7jL34VDpVN+x/qEBALZWpCCk7uZes2ZNvcJ09+7d9Z9qG4Disf3YBfnXl2GSnJ4tQRXLyIKR7aVBVWaGAYBdFEvHxsbqafJqDaF33nlHGjVqpIPR0KFDZf78+cXeSMCZfBN+SoZ/tkOHoDa1/GXlE50JQQBgzzVCR44ckTfffFMXTJtMJodaWZoaIZQW9as4fd0f8uHGo/p171bV5f3+rcXbg3uGAYBd1Qip4ugtW7bIpk2b9CMyMlKaNGkiTz75pB4qA1A4Gdk58sI3e+X73af16ye615fn7mwsrq4unEoAKEFFCkL+/v56IUU1FDZp0iS59dZb9WsAhZeYmqnrgXaduCjuri7y1gMtZUC7IE4lANhrELrnnnt0j9CSJUskPj5eP1RPkKoVAlBw0edTZdSCnXLiQpqUVzPDhoVK5wbMDAMAuy6WXrlypZw/f17fbLVjx46ydu1a3StUo0YN3UsE4OZ2RifKAx9v1SGoZoUy8u3jnQhBAGCEHiELdW+x7OxsyczMlPT0dPn5559l6dKlumgaQP5WRsbpmqDMHJO0DvKX+cPbSpXyXpwyADBCj9AHH3wg999/v1SqVEk6dOggX3/9tR4WW7FihfUGrACuPzNs5voj8szS3ToE3dW8mix55BZCEAAYqUdIBR+1iOKjjz6qh8TU9DQAN58ZNnlFlHwbGadf/6trPXnxribMDAMAowWhXbt2FX9LAAd2KU3NDAuXHdGJ4ubqIq/1aS5DO9S2dbMAwOkVaWhM+e2332TYsGG6WDou7uq/cL/88ks9mwzAX05eSJV+H2/TIaicl7t8NrIdIQgAjByEVC1Qr1699C021GKKGRkZertavfGtt94q7jYChhV+Us0M2ybHz6dKoJ+3fPN4R+nWqIqtmwUA+CdB6I033pC5c+fKf//7X/Hw8LBu79y5s0RERBTlLQGH8+Oe0zL4vzv0gokta/jJynGdpUm1/Jd5BwAYpEbo8OHD0rVr179tV0XTly5dKo52AYaeGfbxpmPy7s+H9es7mgXIzEHB4uP5j1arAADYS49QtWrV5OjRqzeGzE3VB9WrV6842gUYUma2Sa8PZAlBY7vU1atFE4IAwD4V6Z+ojzzyiIwfP14+++wzcXFxkdOnT8v27dvl2WeflVdeeaX4WwkYQNKVLHl8UbhsO3ZB1L1Sp97fXB7uWMfWzQIAFHcQUjdaNZlM0qNHD30nejVM5uXlJc8//7yMHTu2KG8JGFpsYpqMWrhLjiZclrKebjJrSBu5rUlVWzcLAFASQ2OqF+ill16SxMRE2bdvn/z+++96RWlVI1S3bt2ivCVgWBExF6Xv7K06BFXz9ZZlj3UkBAGAIwYhNU1+8uTJ0rZtWz1DbNWqVdKsWTPZv3+/NG7cWGbOnCkTJkwoudYCdmZV1BkZ/MnvciE1U5pV99Uzw5oHstI6ADjk0Jiq/5k3b5707NlTtm3bJv3795dRo0bpHqH3339fv3Zzcyu51gJ2ZMHWaJn64wH9/PYmVeWjwSFS1ouZYQBgJIX6W3v58uXyxRdf6BuuqiGxVq1a6bvP79mzRw+XAc5i0+EEee1/V0PQyE515N/3NtO3zgAAOHAQOnXqlISGhurnLVq00AXSaiiMEARnK4xWd483m0WGdKglr97f3NZNAgCURo1QTk6OeHp6Wl+7u7tLuXLlivqzAcNJz8qRJxZHyKW0LGlV00+m3NfM1k0CAJRWj5BaMXfkyJG6J0hJT0+Xxx57TMqWLZtnv2+//faftAmwW2o4LCouSfx9POTjoW3Ey52aOABwmiA0YsSIPK/V3ecBZ/FN+Cn5akeMqHK4mYNCpGYFH1s3CQBQmkFowYIF//TnAYZ04HSyvPRdlH7+TI9G3EEeAJx5QUXA6W6dsThcMrJN0r1xFXnq9ga2bhIAoJgQhIAbMJnM8uyyPXLyQprU8C8j0wcEiyvT5AHAYRCEgBuYt/m4rD94VjzdXGXOsDZSoexfsyYBAMZHEALyse3YeXn350P6+dQ+zaVVTX/OFQA4GIIQcB3xSeny9NeRYjKLPBRaUwa1C+I8AYADIggB18jMNskTi8Pl/OVMaVrdV17v04LV0wHAQRGEgGtMW31QImIuSXlvd5k7rI2U8WTRRABwVAQhIJcf95yWBVtP6OcfDAiW2pXyrpoOAHAsBCHgT0cTUuTFFXv18ye615c7mgVwbgDAwRGEABG5nJEt//oyXNIyc6RT/Uoy8Y5GnBcAcAIEITg9dTNh1RN07FyqBPh6yYeDQ8TdjV8NAHAG/G0Pp6dqgn7ae0bcXV30HeUrl/Ny+nMCAM6CIASnFnYiUd5adVA/f7l3UwmtXdHWTQIAlCLDBaHZs2dLnTp1xNvbWzp06CA7d+684f7Lly+XJk2a6P1btmwpq1atKrW2wr6dS8mQcV9FSLbJLPe1DpQRnerYukkAgFJmqCC0dOlSmThxokyZMkUiIiKkdevW0qtXL0lISLju/tu2bZPBgwfLmDFjJDIyUvr27asf+/btK/W2w75k55jkqa8j5GxyhjSoWk7+068liyYCgBNyMatKUYNQPUDt2rWTWbNm6dcmk0mCgoLkqaeekkmTJv1t/4EDB0pqaqr873//s2675ZZbJDg4WObOnVugn5mcnCx+fn6SlJQkvr6+xXg0sKX/rD4kc389JmU93eT7J7voMAQAcBwF/fw2TI9QZmamhIeHS8+ePa3bXF1d9evt27df93vU9tz7K6oHKb/9lYyMDH3ycj/gWH7eH69DkPLOQ60JQQDgxAwThM6fPy85OTkSEJB3kTv1Oj4+/rrfo7YXZn9l2rRpOkFaHqrHCY7jxPlUeW7ZHv18TJe60rtVdVs3CQBgQ4YJQqVl8uTJuhvN8oiNjbV1k1BMrmTmyGOLwiUlI1va1q4gk+5uwrkFACfnLgZRuXJlcXNzk7Nnz+bZrl5Xq1btut+jthdmf8XLy0s/4FhUKdxL30XJofgUqVzOU2YPbSMeLJoIAE7PMD1Cnp6eEhoaKhs2bLBuU8XS6nXHjh2v+z1qe+79lXXr1uW7PxzXVztj5NvIOHFzdZGPBreRAF9vWzcJAGAHDNMjpKip8yNGjJC2bdtK+/btZcaMGXpW2KhRo/TXhw8fLjVq1NB1Psr48eOlW7du8v7770vv3r1lyZIlEhYWJp988omNjwSlaU/sJZn6wwH9/IVejaVj/UpcAACA8YKQmg5/7tw5eeWVV3TBs5oGv2bNGmtBdExMjJ5JZtGpUyf56quv5OWXX5b/+7//k4YNG8rKlSulRYsWNjwKlKaLqZnyxOIIycwxyZ3NAuTRrvW4AAAAY64jZAusI2RcOSazjFq4Szb/cU7qVPKRH57qIr7eHrZuFgCgFDjcOkJAYX244YgOQd4erjJnWCghCADwNwQhOKRfDifIhxuP6OfT+rWUptVZFRwA8HcEITic2MQ0mbB0t6hB32G31JIHQmraukkAADtFEIJDSc/K0cXRl9KypHVNP/n3vc1s3SQAgB0jCMGhTP3xgETFJUkFHw/5eFioeLm72bpJAAA7RhCCw1geFitf74wRFxeRmYNCpIZ/GVs3CQBg5whCcAgHTifLyyv36ecTejaSro2q2LpJAAADIAjB8JKuZMnji8MlI9sk3RtXkSdva2DrJgEADIIgBEMzmczy7LI9cvJCmh4KmzEwWFxdXWzdLACAQRCEYGhzNx+T9QfPiqe7q8wdFir+Pp62bhIAwEAIQjCsbUfPy3s/H9bPX7u/ubSs6WfrJgEADIYgBEOKT0qXp76OFJNZpH9oTRnYLsjWTQIAGBBBCIaTmW2SJxaHy4XUTH3rjNf7thAXNWceAIBCIgjBcN5adVAiYi5JeW93mTusjXh7sGgiAKBoCEIwlB/2nJaF207o59MHBEvtSmVt3SQAgIERhGAYR86myKQVe/XzcbfVl57NAmzdJACAwRGEYAiXM7LlsUXhkpaZI50bVJKJdzS2dZMAAA6AIAS7Zzab5cVv9sqxc6lSzddb30fMjUUTAQDFgCAEu/fZ1hPyU9QZ8XBzkdlD20jlcl62bhIAwEEQhGDXdp1IlGmrDurnL/duJqG1K9i6SQAAB0IQgt06l5Ih4xZHSLbJLPe3DpThHWvbukkAAAdDEIJdys4xyVNfR0hCSoY0rFpOpvVryaKJAIBiRxCCXXp37WH5/XiilPV0kznDQqWsl7utmwQAcEAEIdidNfviZd6vx/Xzd/u3lgZVy9m6SQAAB0UQgl2JPp8qzy/fo5+P7VJX7mlZ3dZNAgA4MIIQ7MaVzBx5fFG4pGRkS7s6FeTFu5vYukkAAAdHEILdLJr40ndRcig+Ra8TNGtIG/Fw439PAEDJ4pMGdmHxjhj5NjJOrxg9a0iIBPh627pJAAAnQBCCze2OvSSv/XhAP3/xrsZyS71Ktm4SAMBJEIRgU4mpmXrRxMwck9zVvJo8cms9rggAoNQQhGAzOSazjF8SKXGXrkjdymXlnf6tWDQRAFCqCEKwmZkbjshvR86Lt4erzBnWRny9PbgaAIBSRRCCTfxyKEE+3HBEP/9Pv1bSpJovVwIAUOoIQih1sYlp8szS3fr5w7fUlr4hNbgKAACbIAihVKVn5cgTiyMk6UqWtA7yl5fvbcoVAADYDEEIpWrqjwckKi5JKvh4yMdD24iXuxtXAABgMwQhlJrlYbHy9c4YcXERmTkoRGr4l+HsAwBsiiCEUrH/dJK8vHKffj6xZyPp2qgKZx4AYHMEIZQ4VQ/0+KIIycg2yW2Nq8i42xpw1gEAdoEghBJlMpnl2WW7JSYxTWpWKCPTBwaLq6sLZx0AYBcIQihRc349JusPJoinu6vMHRYq/j6enHEAgN0gCKHEbD16Xt5fe1g/f71Pc2lRw4+zDQCwKwQhlIgzSVfk6a8jxWQWGdC2pgxsV4szDQCwOwQhFLvMbJO+o/yF1ExpVt1XXuvTgrMMALBLBCEUu7dWHZSImEvi6+2u64K8PVg0EQBgnwhCKFbf746ThdtO6OdqhlitSj6cYQCA3SIIodj8cTZFJq2I0s+fvK2B9GgawNkFANg1ghCKxeWMbHlsUbhcycqRzg0qyYQ7GnFmAQB2jyCEf8xsNsuL3+yV4+dSpbqft3w4KETcWDQRAGAABCH8Y59tPSE/RZ0RDzcXmT20jVQq58VZBQAYgmGCUGJiogwdOlR8fX3F399fxowZI5cvX77h93Tv3l1cXFzyPB577LFSa7Mz2HUiUaatOqif//veZtKmVgVbNwkAgAJzF4NQIejMmTOybt06ycrKklGjRsmjjz4qX3311Q2/75FHHpHXXnvN+trHh1lMxSUhJV2vF5RtMkuf4EB5+JbaxfbeAACUBkMEoYMHD8qaNWtk165d0rZtW73to48+knvuuUfee+89CQwMzPd7VfCpVq1aKbbWOWTnmOSpryIlISVDGgWUk2n9WuoeNwAAjMQQQ2Pbt2/Xw2GWEKT07NlTXF1dZceOHTf83sWLF0vlypWlRYsWMnnyZElLSyuFFju+d9celh3RiVLW003mDAsVH09DZGoAAPIwxKdXfHy8VK1aNc82d3d3qVixov5afoYMGSK1a9fWPUZ79+6VF198UQ4fPizffvttvt+TkZGhHxbJycnFdBSOY82+eJn363H9/N3+raV+lXK2bhIAAMYLQpMmTZK33377psNiRaVqiCxatmwp1atXlx49esixY8ekfv361/2eadOmydSpU4v8Mx3d8XOX5bnle/TzR26tK/e0rG7rJgEAYMwg9Oyzz8rIkSNvuE+9evV0jU9CQkKe7dnZ2XomWWHqfzp06KD/PHr0aL5BSA2fTZw4MU+PUFBQUIF/hiNLy8yWxxdF6MUT29epKC/c1cTWTQIAwLhBqEqVKvpxMx07dpRLly5JeHi4hIaG6m0bN24Uk8lkDTcFsXv3bv2n6hnKj5eXl37g74smvvTdPjl8NkUql/OSWUNCxMPNECVmAADkyxCfZE2bNpW77rpLT4XfuXOnbN26VZ588kkZNGiQdcZYXFycNGnSRH9dUcNfr7/+ug5PJ06ckB9++EGGDx8uXbt2lVatWtn4iIxn8Y4Y+S4yTq8YPXtIiFT19bZ1kwAAcI4gZJn9pYKOqvFR0+a7dOkin3zyifXram0hVQhtmRXm6ekp69evlzvvvFN/nxqGe/DBB+XHH3+04VEY0+7YS/Lajwf080l3NZEO9SrZukkAABQLF7Ma80C+VI2Qn5+fJCUl6VWtnU1iaqbc++FvcjopXe5qXk3mDGvDekEAAIf5/DZMjxBKX47JLOOXROoQVLdyWXm3fytCEADAoRCEkK+ZG47Ib0fOSxkPN5k7LFTKe3twtgAADoUghOv65VCCfLjhiH6ubp/RuFp5zhQAwOEQhPA3sYlp8szSq0sNDO9YW/qG1OAsAQAcEkEIeaRn5cjji8Ml6UqWBAf5y0u9m3KGAAAOiyCEPKb+uF/2xSVLBR8P+XhoG/Fyd+MMAQAcFkEIVsvCYuXrnbHi4iLy4eAQCfQvw9kBADg0ghC0/aeT5N8r9+nnz97RSG5tePNbnwAAYHQEIUhSWpa+mWpGtklub1JVnujegLMCAHAKBCEnZzKZ5dnluyUmMU1qVigj0wcEi6uri62bBQBAqSAIObk5vx6T9QcTxNPdVS+a6OfDookAAOdBEHJiW4+el/fXHtbP3+jTQlrU8LN1kwAAKFUEISd1JumKPP11pJjMIgPbBsmAdkG2bhIAAKWOIOSEMrNN8sTiCLmQminNA31lap/mtm4SAAA2QRByQm+tOiiRMZfE19td5gwNFW8PFk0EADgngpCT+X53nCzcdkI/nz4wWGpV8rF1kwAAsBmCkBP542yKTFoRpZ8/dXsD6dE0wNZNAgDApghCTiIlPUse+zJcrmTlSJcGleWZno1s3SQAAGyOIOQEzGazvLhirxw/nyrV/bxl5qBgcWPRRAAACELO4NMt0bIqKl483Fz0HeUrlfOydZMAALAL9Ag5uJ3RiTJt9SH9/JV7m0lIrQq2bhIAAHaDIOTAElLS5cmvIiTHZJa+wYEy7Jbatm4SAAB2hSDkoLJzTPLkV5GSkJIhjQLKyVv9WoqLCzdTBQAgN4KQg3r358N6WKycl7vMGRYqPp7utm4SAAB2hyDkgNbsOyPzNh/Xz999qJXUr1LO1k0CAMAuEYQczPFzl+W55Xv180e71pO7W1a3dZMAALBbBCEHkpaZLY8vipDLGdnSvm5FeaFXY1s3CQAAu0YQcqBFE1/6bp8cPpsiVcp7yazBIeLuxuUFAOBG+KR0EIt2xMh3kXF6xejZQ9pIVV9vWzcJAAC7RxByALtjL8lrP+7Xzyff3UQPiwEAgJsjCBlcYmqmPLEoXLJyzHJ3i2oypktdWzcJAADDIAgZmFoxevySSDmdlC71KpeVdx5qxaKJAAAUAkHIwGau/0N+O3Jeyni46UUTy3t72LpJAAAYCkHIoDYeOisfbjyqn//nwZbSuFp5WzcJAADDIQgZUGximkxYukc/H9GxtvQJrmHrJgEAYEgEIYNJz8qRxxeHS9KVLAkO8peXejezdZMAADAsgpDBvPrDftkXlywVy3rKx0PbiKc7lxAAgKLiU9RAlu2KlSW7YsXVReTDQSES6F/G1k0CAMDQCEIGsS8uSf79/T79/Nk7G0uXhpVt3SQAAAyPIGQASWlZui4oI9skPZpUlce71bd1kwAAcAgEITtnMpll4rLdEpt4RYIqlpEPBgSLqxobAwAA/xhByM7N+fWYbDiUoIui5wwNFT8fFk0EAKC4EITs2JYj5+X9tYf18zf6tJAWNfxs3SQAABwKQchOnUm6Ik8viRSTWWRQuyAZ0C7I1k0CAMDhEITsUGa2SZ5YHKHvLN+ihq+8en9zWzcJAACHRBCyQ2/+dEAiYy6Jr7e7rgvy9nCzdZMAAHBIBCE78/3uOPl8+0n9fMagYAmq6GPrJgEA4LAIQnbkj7MpMmlFlH7+9O0N5PYmAbZuEgAADo0gZCdS0rPksS/D5UpWjtzasLKM79nI1k0CAMDhEYTsgNlslhe+2SvHz6dKoJ+3zBwUIm4smggAQIkjCNmBT7dEy+p98eLh5iKzh7bRd5YHAAAlzzBB6M0335ROnTqJj4+P+Pv7F7in5ZVXXpHq1atLmTJlpGfPnnLkyBGxJzujE2Xa6kP6+Sv3NpOQWhVs3SQAAJyGYYJQZmam9O/fXx5//PECf88777wjH374ocydO1d27NghZcuWlV69ekl6errYg4SUdBn3VYTkmMzyQEgNGXZLbVs3CQAAp+IuBjF16lT958KFCwvcGzRjxgx5+eWXpU+fPnrbF198IQEBAbJy5UoZNGiQ2FJWjkme/CpSzqVkSOOA8vLmAy3ExYWbqQIAUJoM0yNUWNHR0RIfH6+Hwyz8/PykQ4cOsn379ny/LyMjQ5KTk/M8SsK7Px/Ww2LlvNxlzrA24uNpmEwKAIDDcNggpEKQonqAclOvLV+7nmnTpunAZHkEBRX/Pb5Ub5VaLVp1AL3Xv5XUq1Ku2H8GAACw8yA0adIkPRx0o8ehQ1cLiUvL5MmTJSkpyfqIjY0t9p+hjmviHY1k/cRucleL6sX+/gAAoGBsOh7z7LPPysiRI2+4T7169Yr03tWqVdN/nj17Vs8as1Cvg4OD8/0+Ly8v/SgN9ekJAgDAeYNQlSpV9KMk1K1bV4ehDRs2WIOPqvdRs8cKM/MMAAA4LsPUCMXExMju3bv1nzk5Ofq5ely+fNm6T5MmTeS7776zDj8988wz8sYbb8gPP/wgUVFRMnz4cAkMDJS+ffva8EgAAIC9MMxUJbUw4ueff259HRISov/85ZdfpHv37vr54cOHdV2PxQsvvCCpqany6KOPyqVLl6RLly6yZs0a8fb2tsERAAAAe+NiVlOYkC81nKZmj6mA5evry5kCAMCBPr8NMzQGAABQ3AhCAADAaRGEAACA0yIIAQAAp0UQAgAATosgBAAAnBZBCAAAOC2CEAAAcFoEIQAA4LQMc4sNW7EsvK1WqAQAAMZg+dy+2Q00CEI3kZKSov8MCgoqrmsDAABK8XNc3WojP9xr7CZMJpOcPn1aypcvr+9oX5xJVYWr2NhYh72HmaMfo6MfnzMcI8dnfFxDY0suwb9jVE+QCkGBgYHi6pp/JRA9QjehTl7NmjWlpKgL74gfMM50jI5+fM5wjByf8XENjc23hP6OuVFPkAXF0gAAwGkRhAAAgNMiCNmIl5eXTJkyRf/pqBz9GB39+JzhGDk+4+MaGpuXHfwdQ7E0AABwWvQIAQAAp0UQAgAATosgBAAAnBZBCAAAOC2CUAmaPXu21KlTR7y9vaVDhw6yc+fOG+6/fPlyadKkid6/ZcuWsmrVKnGkY1y4cKFenTv3Q32fvdq8ebPcd999elVS1daVK1fe9Hs2bdokbdq00TMgGjRooI/ZUY5PHdu110894uPjxR5NmzZN2rVrp1eFr1q1qvTt21cOHz580+8zyu9hUY7PaL+Dc+bMkVatWlkX2+vYsaOsXr3aIa5fUY7PaNfvWv/5z390m5955hmxp2tIECohS5culYkTJ+ppgREREdK6dWvp1auXJCQkXHf/bdu2yeDBg2XMmDESGRmp/1JTj3379omjHKOiftnPnDljfZw8eVLsVWpqqj4mFfYKIjo6Wnr37i233Xab7N69W/+yjx07Vn7++WdxhOOzUB+2ua+h+hC2R7/++quMGzdOfv/9d1m3bp1kZWXJnXfeqY87P0b6PSzK8Rntd1Ct6q8+PMPDwyUsLExuv/126dOnj+zfv9/w168ox2e065fbrl27ZN68eTr43YhNrqEZJaJ9+/bmcePGWV/n5OSYAwMDzdOmTbvu/gMGDDD37t07z7YOHTqY//WvfznMMS5YsMDs5+dnNiL1q/Ldd9/dcJ8XXnjB3Lx58zzbBg4caO7Vq5fZEY7vl19+0ftdvHjRbEQJCQm6/b/++mu++xjx97Awx2fk30GLChUqmOfPn+9w168gx2fU65eSkmJu2LChed26deZu3bqZx48fn+++triG9AiVgMzMTJ3we/bsmeeeZer19u3br/s9anvu/RXVu5Lf/kY8RuXy5ctSu3ZtfZO9m/3Lx2iMdg2LKjg4WKpXry533HGHbN26VYwiKSlJ/1mxYkWHvIYFOT4j/w7m5OTIkiVLdI+XGkJytOtXkOMz6vUbN26c7i2/9trYyzUkCJWA8+fP6/+pAwIC8mxXr/Orp1DbC7O/EY+xcePG8tlnn8n3338vixYtEpPJJJ06dZJTp06JI8jvGqq7K1+5ckWMToWfuXPnyooVK/RD/UXcvXt3PSxq79T/a2qosnPnztKiRYt89zPa72Fhj8+Iv4NRUVFSrlw5XXf32GOPyXfffSfNmjVzmOtXmOMz4vVbsmSJ/jtC1bQVhC2uIXefR6lR/8rJ/S8d9QvctGlTPW78+uuvcyXsnPpLWD1yX79jx47J9OnT5csvvxR7/xepqjHYsmWLOKKCHp8RfwfV/3Oq5k71eH3zzTcyYsQIXR+VX1gwmsIcn9GuX2xsrIwfP17XsNlzUTdBqARUrlxZ3Nzc5OzZs3m2q9fVqlW77veo7YXZ34jHeC0PDw8JCQmRo0ePiiPI7xqq4sYyZcqII2rfvr3dh4snn3xS/ve//+lZcqo49UaM9ntY2OMz4u+gp6ennoGphIaG6qLbmTNn6g9/R7h+hTk+o12/8PBwPXlGzaS1UCMJ6v/VWbNmSUZGhv4csfU1ZGishP7HVv9Db9iwwbpNdWGq1/mN/artufdXVIq+0Vix0Y7xWuoXQnULqyEXR2C0a1gc1L9k7fX6qRpwFRLUUMPGjRulbt26DnUNi3J8jvA7qP6eUR+gRr9+RTk+o12/Hj166Papvycsj7Zt28rQoUP182tDkM2uYYmVYTu5JUuWmL28vMwLFy40HzhwwPzoo4+a/f39zfHx8frrDz/8sHnSpEnW/bdu3Wp2d3c3v/fee+aDBw+ap0yZYvbw8DBHRUWZHeUYp06dav7555/Nx44dM4eHh5sHDRpk9vb2Nu/fv99srzMdIiMj9UP9qnzwwQf6+cmTJ/XX1bGpY7Q4fvy42cfHx/z888/razh79myzm5ubec2aNWZHOL7p06ebV65caT5y5Ij+/1LN/HB1dTWvX7/ebI8ef/xxPcNm06ZN5jNnzlgfaWlp1n2M/HtYlOMz2u+garuaBRcdHW3eu3evfu3i4mJeu3at4a9fUY7PaNfveq6dNWYP15AgVII++ugjc61atcyenp56qvnvv/+e53+GESNG5Nl/2bJl5kaNGun91TTsn376yexIx/jMM89Y9w0ICDDfc8895oiICLO9skwXv/ZhOSb1pzrGa78nODhYH2O9evX0dFdHOb63337bXL9+ff0Xb8WKFc3du3c3b9y40Wyvrnds6pH7mhj597Aox2e038HRo0eba9eurdtbpUoVc48ePawhwejXryjHZ7TrV5AgZA/X0EX9p+T6mwAAAOwXNUIAAMBpEYQAAIDTIggBAACnRRACAABOiyAEAACcFkEIAAA4LYIQAABwWgQhACWmTp06MmPGjALvv2nTJnFxcZFLly6V6FVZuHCh+Pv7i70ZOXKk9O3b19bNAJwKCyoC0OHjRqZMmSKvvvpqoc/UuXPnpGzZsuLj41Og/TMzMyUxMVECAgJu2qZ/4sqVK5KSkiJVq1bVr9WxrVy5Ut//qDScOHFC3xssMjJSgoODrdvVHcjVGrf2GNIAR8Xd5wHImTNnrGdh6dKl8sorr8jhw4et28qVK2d9rj6o1c0e3d1v/tdHlSpVCn0z39K4U3iZMmX0o7ipIKeOoaj8/PyKtT0Abo6hMQA6fFge6sNY9cZYXh86dEjKly8vq1evltDQUPHy8pItW7bIsWPHpE+fPrr3RgWldu3ayfr16284NKbed/78+fLAAw/oXqKGDRvKDz/8kO/QmGUI6+eff5amTZvqn3PXXXflCW7Z2dny9NNP6/0qVaokL774oowYMeKGQ0y5h8bU86lTp8qePXv0z1YPtU1R7Rg7dqwOdL6+vnL77bfr/SxUT5Lq0VHHpHp4vL299fY1a9ZIly5drG2699579fmysNwpPiQkRP+87t27X3doTN2FXB2b6rlS763ec9euXX87X+pu3equ3uqcdurUKU+IVe297bbb9DVUx6CuYVhYGP/XA38iCAEokEmTJsl//vMfOXjwoLRq1UouX74s99xzj/4QVkM8KqDcd999EhMTc8P3UaFjwIABsnfvXv39Q4cO1cNh+UlLS5P33ntPvvzyS9m8ebN+/+eee8769bffflsWL14sCxYskK1bt0pycrIe5iqogQMHyrPPPivNmzfXAUs91Dalf//+kpCQoENgeHi4tGnTRnr06JGnvUePHpUVK1bIt99+ax1aS01NlYkTJ+rAoc6Pq6urDn8mk0l/fefOnfpPFRzVz1Pfez0vvPCCfu/PP/9cIiIipEGDBtKrV6+/na+XXnpJ3n//ff3zVE/d6NGjrV9T57dmzZo6QKljUNfRw8OjwOcHcHglektXAIaj7l7u5+f3t7vUr1y58qbfq+4U/dFHH1lfqztrT58+3fpavc/LL79sfX358mW9bfXq1Xl+1sWLF61tUa+PHj1q/Z7Zs2frO29bqOfvvvuu9XV2dra+Q3efPn0KfIxTpkwxt27dOs8+v/32m9nX19ecnp6eZ3v9+vXN8+bNs36fh4eHOSEh4Ybn5dy5c/o4oqKi9Ovo6Gj9OjIyMs9+6i7clnarc6Pee/HixdavZ2ZmmgMDA83vvPNOnvO1fv166z7qTt1q25UrV/Tr8uXLmxcuXHjD9gHOjB4hAAWihl5yUz1CqmdGDVmpISA1bKV6i27WI6R6kyxUIbUarlG9LvlRwz3169e3vq5evbp1f1VcfPbsWWnfvr31625ubnr4559SQ0rqGNXQljo2yyM6OjrPMFft2rX/Vgt15MgRGTx4sNSrV08fnxoiVG52bnJTPyMrK0s6d+5s3aZ6ctSxqvOc3zlV50exnCPVM6WG93r27Kl79HK3HQDF0gAKSIWW3FQIWrdunR62UkM2qvj4oYce0gXDN3LtsIyqcbEMGRV0/6udSyVLhSAVKlQdzrVyz+q69rwoaohQBaT//ve/EhgYqI+vRYsWNz03RZX7HFlm21nOqapjGjJkiPz00096iE/NAFyyZIkeqgNAjRCAIlL1OKq4V32gtmzZUhdWq2nhpUkVdqti7dwFxGpGm6qnKQw100t9X26qHig+Pl7X3Kigl/tRuXLlfN/rwoULulj55Zdf1vVEqsfs4sWLf/t5lrbmR/WCqf3UebZQPUTqWJs1a1ao42vUqJFMmDBB1q5dK/369dP1VACuYmgMQJGoGV+WAmE1jKR6HW7Us1NSnnrqKZk2bZp8//33OoCMHz9eB4/CrEOkhq7UkJc6lvPnz+vZWmooqWPHjnoWlwoQKuRt27ZNFybfaNZVhQoV9HDaJ598ogupN27cqIenclOzwFQPmppdpob21BDftVRP0+OPPy7PP/+83u/AgQPyyCOP6OLxMWPGFHi9pCeffFL3ap08eVKHKhWkVDgDcBVBCECRfPDBB/pDX03XVkNBajaT6kUpbWq6vKrHGT58uA4uqo5HtcUylb0gHnzwQT3rTU0zV/U+X3/9tQ5Sq1atkq5du8qoUaN0r8qgQYN0oFC9UPlRM8TU0JOaoaWGw1RPzLvvvptnH9XL9OGHH8q8efP00JlahuB6VE2PatvDDz+sz60KVmopAXXeC0LVS6keKnVuVPvVbL27775bz9wDcBUrSwNwKKpXSvV4qA/9119/3dbNAWDnWFkagKGpHho1dNWtWzc9pDVr1iw9zKWG6gDgZhgaA2BoaihKrQStVrZWU82joqL0QoXUwQAoCIbGAACA06JHCAAAOC2CEAAAcFoEIQAA4LQIQgAAwGkRhAAAgNMiCAEAAKdFEAIAAE6LIAQAAJwWQQgAAIiz+n+Gyp5R69rF9gAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(episode_reward_mean_list)\n",
        "plt.xlabel(\"Training iterations\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Episode reward mean\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to navigation.gif\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "expected bytes, NoneType found",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m imageio.get_writer(video_path, fps=\u001b[32m20\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m env.frames:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winds\\Desktop\\ML\\RL\\RL-Georgia Tech\\.venv\\Lib\\site-packages\\imageio\\v2.py:226\u001b[39m, in \u001b[36mLegacyWriter.append_data\u001b[39m\u001b[34m(self, im, meta)\u001b[39m\n\u001b[32m    214\u001b[39m     warnings.warn(\n\u001b[32m    215\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mV3 Plugins currently don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have a uniform way to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    216\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m write metadata, so any metadata is ignored.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    217\u001b[39m     )\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# total_meta = dict()\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# if meta is None:\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m#     meta = {}\u001b[39;00m\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# if hasattr(im, \"meta\") and isinstance(im.meta, dict):\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[38;5;66;03m#     total_meta.update(im.meta)\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[38;5;66;03m# total_meta.update(meta)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_args\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winds\\Desktop\\ML\\RL\\RL-Georgia Tech\\.venv\\Lib\\site-packages\\imageio\\plugins\\pyav.py:674\u001b[39m, in \u001b[36mPyAVPlugin.write\u001b[39m\u001b[34m(self, ndimage, codec, is_batch, fps, in_pixel_format, out_pixel_format, filter_sequence, filter_graph)\u001b[39m\n\u001b[32m    671\u001b[39m     ndimage = np.asarray(ndimage)\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._video_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m674\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_video_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_pixel_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[38;5;28mself\u001b[39m.set_video_filter(filter_sequence, filter_graph)\n\u001b[32m    678\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m ndimage:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winds\\Desktop\\ML\\RL\\RL-Georgia Tech\\.venv\\Lib\\site-packages\\imageio\\plugins\\pyav.py:887\u001b[39m, in \u001b[36mPyAVPlugin.init_video_stream\u001b[39m\u001b[34m(self, codec, fps, pixel_format, max_keyframe_interval, force_keyframes)\u001b[39m\n\u001b[32m    884\u001b[39m \u001b[38;5;66;03m# It may introduce `OverflowError` if `fps` is float\u001b[39;00m\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# which is a legacy issue of `pyav`: https://github.com/PyAV-Org/PyAV/issues/242\u001b[39;00m\n\u001b[32m    886\u001b[39m fps = Fraction.from_float(fps).limit_denominator(\u001b[32m65535\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_container\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    888\u001b[39m stream.time_base = Fraction(\u001b[32m1\u001b[39m / fps).limit_denominator(\u001b[38;5;28mint\u001b[39m(\u001b[32m2\u001b[39m**\u001b[32m16\u001b[39m - \u001b[32m1\u001b[39m))\n\u001b[32m    889\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pixel_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winds\\Desktop\\ML\\RL\\RL-Georgia Tech\\.venv\\Lib\\site-packages\\av\\container\\output.py:60\u001b[39m, in \u001b[36mav.container.output.OutputContainer.add_stream\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winds\\Desktop\\ML\\RL\\RL-Georgia Tech\\.venv\\Lib\\site-packages\\av\\codec\\codec.pyx:85\u001b[39m, in \u001b[36mav.codec.codec.Codec.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mTypeError\u001b[39m: expected bytes, NoneType found"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Frame collector callback\n",
        "def rendering_callback(env, td):\n",
        "    env.frames.append(Image.fromarray(env.render(mode=\"rgb_array\")))\n",
        "\n",
        "# Prepare environment for rendering\n",
        "env.frames = []\n",
        "\n",
        "# Run the rollout and collect frames\n",
        "with torch.no_grad():\n",
        "    env.rollout(\n",
        "        max_steps=max_steps,\n",
        "        policy=policy,\n",
        "        callback=rendering_callback,\n",
        "        auto_cast_to_device=True,\n",
        "        break_when_any_done=True,  # or False if you want full rollout\n",
        "    )\n",
        "\n",
        "# Save the video/gif\n",
        "gif_path = f\"{scenario_name}.gif\"\n",
        "env.frames[0].save(\n",
        "    gif_path,\n",
        "    save_all=True,\n",
        "    append_images=env.frames[1:],\n",
        "    duration=50,  # ms per frame\n",
        "    loop=0,\n",
        ")\n",
        "\n",
        "print(f\"Saved to {gif_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Render\n",
        "\n",
        "If you are running this in a machine with GUI, you can render the trained policy by running:\n",
        "\n",
        "```python\n",
        "with torch.no_grad():\n",
        "   env.rollout(\n",
        "       max_steps=max_steps,\n",
        "       policy=policy,\n",
        "       callback=lambda env, _: env.render(),\n",
        "       auto_cast_to_device=True,\n",
        "       break_when_any_done=False,\n",
        "   )\n",
        "```\n",
        "If you are running this in Google Colab, you can render the trained policy by running:\n",
        "\n",
        "```bash\n",
        "!apt-get update\n",
        "!apt-get install -y x11-utils\n",
        "!apt-get install -y xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "```\n",
        "```python\n",
        "import pyvirtualdisplay\n",
        "display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "display.start()\n",
        "from PIL import Image\n",
        "\n",
        "def rendering_callback(env, td):\n",
        "    env.frames.append(Image.fromarray(env.render(mode=\"rgb_array\")))\n",
        "env.frames = []\n",
        "with torch.no_grad():\n",
        "   env.rollout(\n",
        "       max_steps=max_steps,\n",
        "       policy=policy,\n",
        "       callback=rendering_callback,\n",
        "       auto_cast_to_device=True,\n",
        "       break_when_any_done=False,\n",
        "   )\n",
        "env.frames[0].save(\n",
        "    f\"{scenario_name}.gif\",\n",
        "    save_all=True,\n",
        "    append_images=env.frames[1:],\n",
        "   duration=3,\n",
        "   loop=0,\n",
        ")\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(open(f\"{scenario_name}.gif\", \"rb\").read())\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion and next steps\n",
        "\n",
        "In this tutorial, we have seen:\n",
        "\n",
        "- How to create a multi-agent environment in TorchRL, how its specs work, and how it integrates with the library;\n",
        "- How you use GPU vectorized environments in TorchRL;\n",
        "- How to create different multi-agent network architectures in TorchRL (e.g., using parameter sharing, centralised critic)\n",
        "- How we can use :class:`tensordict.TensorDict` to carry multi-agent data;\n",
        "- How we can tie all the library components (collectors, modules, replay buffers, and losses) in a multi-agent MAPPO/IPPO training loop.\n",
        "\n",
        "Now that you are proficient with multi-agent DDPG, you can check out all the TorchRL multi-agent implementations in the\n",
        "GitHub repository.\n",
        "These are code-only scripts of many popular MARL algorithms such as the ones seen in this tutorial,\n",
        "QMIX, MADDPG, IQL, and many more!\n",
        "\n",
        "You can also check out our other multi-agent tutorial on how to train competitive\n",
        "MADDPG/IDDPG in PettingZoo/VMAS with multiple agent groups: :doc:`/tutorials/multiagent_competitive_ddpg`.\n",
        "\n",
        "If you are interested in creating or wrapping your own multi-agent environments in TorchRL,\n",
        "you can check out the dedicated\n",
        "`doc section <MARL-environment-API>`.\n",
        "\n",
        "Finally, you can modify the parameters of this tutorial to try many other configurations and scenarios\n",
        "to become a MARL master.\n",
        "Here are a few videos of some possible scenarios you can try in VMAS.\n",
        "\n",
        ".. figure:: https://github.com/matteobettini/vmas-media/blob/main/media/vmas_scenarios_more.gif?raw=true\n",
        "   :alt: VMAS scenarios\n",
        "\n",
        "   Scenarios available in [VMAS](https://github.com/proroklab/VectorizedMultiAgentSimulator)_\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
