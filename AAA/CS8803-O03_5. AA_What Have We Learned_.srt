1
00:00:00,090 --> 00:00:02,810
So in this section on advanced
algorithmic analysis,

2
00:00:02,810 --> 00:00:03,730
what have we learned?

3
00:00:03,730 --> 00:00:06,397
>> Well we learned a bunch of stuff.

4
00:00:06,397 --> 00:00:10,900
It's a compact and dense lesson.

5
00:00:10,900 --> 00:00:11,850
I think that's pretty good.

6
00:00:11,850 --> 00:00:13,270
>> Oh that was a math joke, wasn't it?

7
00:00:13,270 --> 00:00:14,880
>> Yes it was a math joke.

8
00:00:14,880 --> 00:00:17,375
Well the first thing we learned
is that you like alliteration.

9
00:00:17,375 --> 00:00:19,840
>> [LAUGH]
>> This whole section was

10
00:00:19,840 --> 00:00:21,790
advanced algorithmic analysis.

11
00:00:21,790 --> 00:00:22,764
>> Okay.
>> Mm-hm.

12
00:00:22,764 --> 00:00:26,240
And in particular we talked about
three things, all ending in I.

13
00:00:26,240 --> 00:00:28,580
VI, LI and PI.

14
00:00:28,580 --> 00:00:31,200
>> [LAUGH] Yeah there's
a problem with LI though.

15
00:00:31,200 --> 00:00:32,780
>> Yeah that it's not really a thing.

16
00:00:32,780 --> 00:00:35,290
>> Yeah, it's LP in your programming.

17
00:00:35,290 --> 00:00:37,730
>> But then it wouldn't end in an I.

18
00:00:37,730 --> 00:00:39,550
>> I agree with that, but
at least it ends with P and

19
00:00:39,550 --> 00:00:40,830
the other one starts with P.

20
00:00:40,830 --> 00:00:42,190
There is some over lap.

21
00:00:42,190 --> 00:00:43,690
All of these things
overlap with each other.

22
00:00:43,690 --> 00:00:47,620
>> If you call it VLP for
very linear programming.

23
00:00:47,620 --> 00:00:48,920
>> No, It's just linear programming.

24
00:00:48,920 --> 00:00:50,680
Let's just go with that.

25
00:00:50,680 --> 00:00:52,510
>> Okay.
Very linear is still linear.

26
00:00:52,510 --> 00:00:53,930
In any case, all right.

27
00:00:53,930 --> 00:00:54,720
So what did we learn?

28
00:00:54,720 --> 00:00:57,260
We learned a little bit
about each of those things.

29
00:00:57,260 --> 00:01:01,270
So in particular for
value iteration, we learned that it

30
00:01:01,270 --> 00:01:06,370
converges to optimal in a finite number
of steps, or it gets bound towards that.

31
00:01:06,370 --> 00:01:07,780
>> Okay.
Can we be more precise there?

32
00:01:07,780 --> 00:01:10,230
The value function doesn't converge
in a finite number of steps.

33
00:01:10,230 --> 00:01:11,430
>> But the policy does.

34
00:01:11,430 --> 00:01:12,960
>> The greedy policy does, yeah.

35
00:01:12,960 --> 00:01:14,410
>> Right.
But really, that's what we want.

36
00:01:14,410 --> 00:01:15,090
>> Yeah.

37
00:01:15,090 --> 00:01:16,490
I agree with that.

38
00:01:16,490 --> 00:01:17,740
>> Sure.
But that's a good point.

39
00:01:17,740 --> 00:01:21,270
It's not that the value function
itself has to, because otherwise

40
00:01:21,270 --> 00:01:23,860
we go all the way back to 'em' and
have that conversation again.

41
00:01:23,860 --> 00:01:27,960
But that the policy itself
will stop changing.

42
00:01:27,960 --> 00:01:31,960
The greedy policy will stop changing
at some point and in finite time.

43
00:01:31,960 --> 00:01:32,690
>> Right.

44
00:01:32,690 --> 00:01:34,230
And it will be optimal at that point.

45
00:01:34,230 --> 00:01:37,750
Yeah, and so the value function itself
doesn't necessarily converge, but

46
00:01:37,750 --> 00:01:40,960
it's going to get boundedly
close to the solution.

47
00:01:40,960 --> 00:01:45,620
So that is to say,
if we want to be epsilon close to

48
00:01:45,620 --> 00:01:50,520
the solution to the Belmont equation,
we can define how many iterations of

49
00:01:50,520 --> 00:01:54,630
value iteration we would need to
accomplish that degree of approximation.

50
00:01:54,630 --> 00:01:56,120
>> Right.
And that's fine.

51
00:01:56,120 --> 00:01:57,700
Because in the end that's interesting,

52
00:01:57,700 --> 00:02:00,830
but what's really interesting is that
we get a policy that we can act on.

53
00:02:00,830 --> 00:02:02,130
>> Because we want to maximize reward.

54
00:02:02,130 --> 00:02:03,260
That's just what we are about.

55
00:02:03,260 --> 00:02:04,458
>> That's what we are all about, really.

56
00:02:04,458 --> 00:02:07,500
So let's see.
Then we talked about linear programming,

57
00:02:07,500 --> 00:02:11,970
which is another way to
think about solving MVPs.

58
00:02:11,970 --> 00:02:16,500
What I remember most about it
is you actually define the dual.

59
00:02:16,500 --> 00:02:17,000
>> Okay.

60
00:02:17,000 --> 00:02:21,960
>> That was really cool because it
allowed us to solve the problem and

61
00:02:21,960 --> 00:02:24,970
like so many things that we do,
it was a call back to our first class.

62
00:02:24,970 --> 00:02:27,637
And now I actually understand how
support vector machines work.

63
00:02:27,637 --> 00:02:30,300
>> [LAUGH] Okay.

64
00:02:30,300 --> 00:02:33,600
I think you did that lecture, so
I was hoping you understood it already.

65
00:02:33,600 --> 00:02:34,717
>> Well,
there was just a point where you say,

66
00:02:34,717 --> 00:02:37,940
well or could you solve the dual,
and then I rapidly wave my hands and

67
00:02:37,940 --> 00:02:39,740
said don't worry about it,
it's a dual, it's a thing.

68
00:02:39,740 --> 00:02:41,420
>> Okay.
>> But now we talked about the dual, so

69
00:02:41,420 --> 00:02:42,170
I think that that's good.

70
00:02:42,170 --> 00:02:45,640
>> I feel like the concept of dual
is somehow relevant to superheroes.

71
00:02:45,640 --> 00:02:46,280
>> You think so?

72
00:02:46,280 --> 00:02:46,830
>> I don't know.

73
00:02:46,830 --> 00:02:50,820
You know way more about comic books
than I do, but isn't there like every

74
00:02:50,820 --> 00:02:56,130
superhero has a dual,
like anti-Batman or the dark Spiderman?

75
00:02:56,130 --> 00:02:56,670
>> Venom?

76
00:02:56,670 --> 00:03:03,160
So you're saying that primal and
duals are like nemesis.

77
00:03:03,160 --> 00:03:05,510
>> Yeah, or
mirror images of each other in some way.

78
00:03:05,510 --> 00:03:09,440
>> So the flow,
the sort of policy flow idea,

79
00:03:09,440 --> 00:03:13,540
is like the nemesis of the value idea,
or the mirror image of it.

80
00:03:13,540 --> 00:03:14,140
>> Yeah, sure.

81
00:03:14,140 --> 00:03:14,740
>> Let's go with that.

82
00:03:14,740 --> 00:03:17,440
I was thinking more of a hip-hop
analogy, but I think that's pretty good.

83
00:03:17,440 --> 00:03:19,040
>> Oh, flow is a hip-hop thing.

84
00:03:19,040 --> 00:03:19,970
>> It's a hip-hop thing.

85
00:03:19,970 --> 00:03:21,430
>> And values.

86
00:03:21,430 --> 00:03:23,010
People in hip hop have values.

87
00:03:23,010 --> 00:03:25,310
>> That's right, so
it just follows in one step.

88
00:03:25,310 --> 00:03:26,660
Okay, speaking in one step.

89
00:03:26,660 --> 00:03:30,980
So, then there's policy iteration,
which I thought was kind of cool.

90
00:03:30,980 --> 00:03:32,940
We got a chance to prove
a bunch of things about it.

91
00:03:32,940 --> 00:03:34,860
And I think the first thing
we talked about there or

92
00:03:34,860 --> 00:03:38,410
somewhere in that mix was domination,
which turned out to be really cool.

93
00:03:38,410 --> 00:03:41,220
>> Yeah and that comes up actually
in all of the algorithms.

94
00:03:41,220 --> 00:03:42,650
Value iteration has some of that.

95
00:03:42,650 --> 00:03:44,650
Linear programming has some of that.

96
00:03:44,650 --> 00:03:46,030
And policy iteration.

97
00:03:46,030 --> 00:03:48,970
The proof of policy iteration
depends significantly on that.

98
00:03:48,970 --> 00:03:50,925
>> Right.
Speaking of things the proof of policy

99
00:03:50,925 --> 00:03:55,800
iteration depends on, we also talked
about value non deprovement, or

100
00:03:55,800 --> 00:03:59,090
value improvement as I believe is
the non-technical term for it.

101
00:03:59,090 --> 00:04:02,880
>> That's right and we argued that we'd
actually get strict value improvement

102
00:04:02,880 --> 00:04:05,320
any time that we didn't have
a policy that was already optimal.

103
00:04:05,320 --> 00:04:09,970
If it was already optimal then a step of
policy iteration resulted in value non

104
00:04:09,970 --> 00:04:14,700
deprovement which is just our broken
way of saying it doesn't get worse.

105
00:04:14,700 --> 00:04:16,690
It might not get better,
but it doesn't get worse.

106
00:04:16,690 --> 00:04:17,894
>> Yeah.
It's like a monotonically

107
00:04:17,894 --> 00:04:19,700
non-decreasing function.

108
00:04:19,700 --> 00:04:20,870
But the other thing is,

109
00:04:20,870 --> 00:04:23,550
we had a discussion that I think
is actually pretty important,

110
00:04:23,550 --> 00:04:27,260
that when we talk about things like
domination, you writing them down for

111
00:04:27,260 --> 00:04:30,890
good notational reasons, talking
about an entire value function, but

112
00:04:30,890 --> 00:04:35,380
really this is about things have to
also be true on a state by state basis.

113
00:04:35,380 --> 00:04:37,160
>> That's right.
>> Because if that weren't true,

114
00:04:37,160 --> 00:04:40,690
then in principle, you could cycle, and
then you get stuck in local optimum.

115
00:04:40,690 --> 00:04:44,190
But here,
we don't get stuck in local optimum,

116
00:04:44,190 --> 00:04:47,250
because things also dominate
on a state by state basis.

117
00:04:47,250 --> 00:04:49,340
>> Cool.
>> Kind of neat.

118
00:04:49,340 --> 00:04:50,960
We also talked about monotonicity.

119
00:04:50,960 --> 00:04:52,830
>> And
that you can contract it from kissing.

120
00:04:52,830 --> 00:04:53,620
>> Yeah.

121
00:04:53,620 --> 00:04:54,780
Oh, you said contract.

122
00:04:54,780 --> 00:04:55,980
Very well done, Mike.

123
00:04:55,980 --> 00:04:57,260
>> Oh, yeah.

124
00:04:57,260 --> 00:04:58,716
That was pun intentional.

125
00:04:58,716 --> 00:05:00,066
Sorry.
>> Pun intentional.

126
00:05:00,066 --> 00:05:04,060
[LAUGH] Okay and really when you
put all these things together.

127
00:05:04,060 --> 00:05:08,285
I think the real thing we learned in
this lesson is that proofs do not suck.

128
00:05:08,285 --> 00:05:12,120
>> [LAUGH] So that's great.

129
00:05:12,120 --> 00:05:16,880
Because if I recall we started our last
class with a little discussion about how

130
00:05:16,880 --> 00:05:19,560
I don't mind proofs as much as you do.

131
00:05:19,560 --> 00:05:20,622
>> That's right.
Speaking of proof you

132
00:05:20,622 --> 00:05:22,100
forgot the s on proofs.

133
00:05:22,100 --> 00:05:25,620
>> See, I did a proof of your proof.

134
00:05:25,620 --> 00:05:26,990
That was pretty good.

135
00:05:26,990 --> 00:05:27,840
>> You proof read it?

136
00:05:27,840 --> 00:05:28,570
>> Yes.

137
00:05:28,570 --> 00:05:29,530
Yes.

138
00:05:29,530 --> 00:05:31,430
>> Nice.
>> I feel like we have converged.

139
00:05:31,430 --> 00:05:34,570
>> [LAUGH] And
that's what this was all about.

140
00:05:34,570 --> 00:05:36,150
>> That's what this is all about.

141
00:05:36,150 --> 00:05:38,730
>> Yeah, there's something
very satisfying about getting

142
00:05:38,730 --> 00:05:39,800
to the convergence point.

143
00:05:39,800 --> 00:05:40,530
>> Very nice.
Very nice.

144
00:05:40,530 --> 00:05:41,122
Okay well,

145
00:05:41,122 --> 00:05:44,775
I think that's everything that I was
capable of learning in one lesson.

146
00:05:44,775 --> 00:05:49,200
>> [LAUGH] Yeah, all right, well so next
time we'll start in with something else.

147
00:05:49,200 --> 00:05:52,380
I think we're going to talk about how
we can modify reward functions and

148
00:05:52,380 --> 00:05:53,680
get different things out of them.

149
00:05:53,680 --> 00:05:54,860
>> Oh yeah, that's my favorite.

150
00:05:54,860 --> 00:05:54,860
Excellent.
Well, then I will see you then.

