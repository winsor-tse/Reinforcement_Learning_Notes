# ğŸ§  Temporal Difference Learning & Eligibility Traces â€“ Study Notes

---

## ğŸ“Œ Overview: Temporal Difference (TD) Learning

Temporal Difference (TD) learning is a core method in reinforcement learning that updates value estimates based on the difference between successive predictions (TD error), without waiting for the final outcome.

---

## ğŸ§® What Is an Eligibility Trace (\( e_t \))?

In TD learning, the letter \( e \) (often \( e_t \) or \( E \)) represents the **eligibility trace**.
ğŸ“Œ **Note:** This is *not* related to the **probability of a state transition**.

---

### âœ… Definition

An **eligibility trace** is a temporary, decaying memory that tracks **how recently and frequently** a particular state (or state-action pair) has been visited **during an episode**.

---

### ğŸ¯ Purpose: Solving the Credit Assignment Problem

When a TD error (or reward) is received, we want to assign credit not just to the immediately preceding state, but also to **earlier states/actions** that contributed.

The eligibility trace helps determine **how much each past state/action should be updated**, based on:

- **Recency** (how long ago it was visited),
- **Frequency** (how often itâ€™s been visited in the current episode).

---

## ğŸ” Eligibility Traces: TD(Î») vs TD(0) vs Monte Carlo

The decay of the eligibility trace is governed by **Î» (lambda)**.


| Î» Value              | Behavior                                                                 |
| ----------------------- | -------------------------------------------------------------------------- |
| \( \lambda = 0 \)     | Only the immediately previous state gets updated (**TD(0)**).            |
| \( \lambda = 1 \)     | All states from the episode get updated equally (**Monte Carlo-style**). |
| \( 0 < \lambda < 1 \) | A**blend**: more recent states get more credit (exponentially decayed).  |

âœ… This blending allows TD(Î») to balance **bias and variance** in value estimates.

---

## ğŸ“ Eligibility Trace Update Rule

### Formula:



### Terms:

- \( e_t \): Eligibility trace at time \( t \)
- \( \gamma \): Discount factor
- \( \lambda \): Trace decay parameter
- \( \nabla_w V(S_t, w) \): Gradient of value function w.r.t. weights \( w \)
  - In tabular methods, this is typically 1 for the current state

---

## ğŸ”„ Weight Update Rule in TD(Î»)

After computing the TD error \( \delta_t \), the weights are updated as:

w_{t+1} = w_t + \alpha \delta_t e_t

Where:

- \( \alpha \): Learning rate
- \( \delta_t \): TD error = \( R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \)

âœ… This allows **a single TD error** to update **many previous states**, weighted by their eligibility.

---

## ğŸ§  How Does Decaying Eligibility Help Credit Assignment?

Compared to one-step TD (TD(0)), eligibility traces **speed up learning** by allowing a single reward to **impact multiple prior states/actions**.

### Example:

Imagine an agent in a grid-world that gets a reward only after 10 steps.

- **TD(0)** will update only the 9th state â†’ it takes many episodes for the first state to get useful feedback.
- **TD(Î»)** will assign some of the TD error to **all previous steps**, proportionally to their eligibility â€” speeding up learning significantly.

---

## ğŸš€ Example Use Case for Eligibility Traces

**CartPole (Classic Control):**

- The agent balances a pole by applying left/right forces.
- A reward is given for every timestep the pole remains upright.
- **Eligibility traces** allow the agent to more quickly associate early actions with success/failure, instead of waiting for the terminal reward.

---

# ğŸ§  Eligibility Traces in Deep Reinforcement Learning (Deep RL)

---

## âŒ Why Classic Eligibility Traces Arenâ€™t Used in Deep RL

Despite their usefulness in tabular settings, **standard online TD(Î»)** is **rarely used in Deep RL** due to:

### ğŸ” 1. Experience Replay Breaks Sequentiality

- Algorithms like **DQN** use a **replay buffer** that stores transitions and samples them randomly.
- This **i.i.d. sampling** improves stability of neural network training.
- But it **breaks the time sequence** required for traditional eligibility traces.

### ğŸ§® 2. Computational Cost

- Deep networks have **millions of parameters**.
- Maintaining and updating an eligibility trace for each parameter is **computationally expensive**.

---

## ğŸ§  What Does Deep RL Use Instead?

### âœ… One-Step TD (TD(0)) â€” Used in DQN

- DQN uses the following TD(0)-style update:

Q(S_t, A_t) â† Q(S_t, A_t) + Î± [R_{t+1} + Î³ max_a Q(S_{t+1}, a) âˆ’ Q(S_t, A_t)]


- This is **TD(0)** â€” only the immediate reward and next state influence the update.

---

## ğŸ” Alternatives Inspired by Eligibility Traces

### ğŸ”¹ N-step Returns

- Instead of 1-step TD, some algorithms use **fixed-length returns** (e.g., 3-step, 5-step):
- This is **TD(0)** â€” only the immediate reward and next state influence the update.

---

## ğŸ” Alternatives Inspired by Eligibility Traces

### ğŸ”¹ N-step Returns

- Instead of 1-step TD, some algorithms use **fixed-length returns** (e.g., 3-step, 5-step):

R_{t:t+n} = R_{t+1} + Î³R_{t+2} + ... + Î³^{n-1}R_{t+n} + Î³^n V(S_{t+n})

- Approximates Î»-returns without maintaining full eligibility traces.
- Used in:
  - **A3C**, **Rainbow DQN**, **IMPALA**, etc.

### ğŸ”¹ Generalized Advantage Estimation (GAE)

- Used in **actor-critic** methods like PPO.
- Blends TD and Monte Carlo advantages:

AÌ‚_t = âˆ‘*{l=0}^{âˆ} (Î³Î»)^l Î´* {t+l}

- Conceptually similar to TD(Î»), but adapted for use with **policy gradients** and **neural networks**.

### ğŸ”¹ Offline Î»-Returns

- Compute Î»-returns **after trajectories are collected**.
- Compatible with replay buffers.
- Used in **Retrace(Î»)** (e.g., ACER algorithm).

---

## ğŸ“š Summary: TD(Î») vs Deep RL Practices


| Feature          | Classic TD(Î»)            | Deep RL (e.g., DQN)       |
| ------------------ | --------------------------- | --------------------------- |
| TD Type          | Multi-step with decay     | One-step TD (TD(0))       |
| Update Style     | Online, sequential        | Offline, random-sampled   |
| Memory Mechanism | Eligibility trace vector  | Experience replay buffer  |
| Use of Î»        | Direct credit decay       | Indirect (via n-step/GAE) |
| Ideal For        | Tabular RL, small domains | Deep RL, high-dimensional |

---

## ğŸ§  Final Notes

- Traditional **eligibility traces** are powerful tools for **fast credit assignment** in **tabular** environments.
- In **Deep RL**, the same idea is preserved through:

  - **n-step returns**
  - **GAE**
  - **Offline Î»-returns**
- The **TD(Î») principle** (blending TD and Monte Carlo) is **still influential**, even if the **exact implementation** changes.
